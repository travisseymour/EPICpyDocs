{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the EPICpy Docs You can find EPICpy related content at one of these locations: Overview Detailed Documentation @ GitHub Pages (You're Reading It Now!) EPICpy Article Development Environment @ Github If you want to jump directly to a step-by-step example of how you use EPICpy to do EPIC modeling (with an admittedly toy-example task), see the Example page. If you are looking for a quick test to make sure EPICpy is working correctly on your computer, go to the An EPIC Model of the Human Data section on the Example page and follow the directions under Initial Model Process Diagram . What These Docs ARE This is the documentation for EPICpy , a tool for simulating human performance tasks using EPIC (a computational cognitive architecture created by David Kieras and David Meyer) . For a brief overview of EPIC and EPICpy, see the sections below. What These Docs ARE NOT This documentation is not a detailed introduction to EPIC, device programming, perceptual encoder programming, or production rule programming . To learn more about EPIC and how it works, please see the documents in the Epic Resources section (in particular, the first three). What is EPIC EPIC Schematic (Meyer & Kieras 1997a) EPIC is a computational cognitive architecture that specifies a theory of human performance and a facility to create individual task models constrained by that theory. EPIC compacts decades of psychological theory and findings into a rich set of processors, including those that model sensory, perceptual, motor, cognitive, and memory subsystems of the human mind. EPIC is particularly suited to stimulus-response type human performance tasks including those involving multitasking and task-switching. The goal of creating model simulations in EPIC is typically to either verify your understanding of human performance in some task for which you already have human data, or to predict human performance in some speculative task. Either way, this typically involves comparing simulated data from EPIC (e.g., simulated response times, accuracy rates, eye movements, etc.) to human data using a similar task. Over many publications (see \"Various Published Uses of EPIC\" within the EPIC Resources section , Kiears, Meyer, and their colleagues have demonstrated that if you 1) create a simulated task within EPIC (called a \"device\") sufficiently close to the corresponding human task and 2) create a simulated task strategy (embodied within a set of \"production rules\" and model configurations) sufficiently close to the strategy humans used when performing the task, then EPIC will produce a set of simulated performance data that follows very close (often nearly identical) to the observed human data. What is EPICapp (the original tool available for EPIC modeling) The EPIC architecture is a C++ code library (called EPIClib ) capable of simulating human performance according to a particular theory of human performance using a massive set of defaults chosen carefully from various past literatures (with a few guesses where none exists). Although most EPIC modellers only use EPICLib, It is possible to alter the EPIC architecture itself to add new processors, change how various components operate, and so on. This requires an advanced level of C++ programming skill (including mastery of various design patterns, and abstraction). EPICLib module is not generally usable on its own and so David Kieras created EPICapp. EPICapp is a Graphical User Interface (GUI) based application that serves as a controller for EPIC simulations, as well as offering a set of views for visualizing simulation progress. It allows researchers to: Create simulated task devices in C++ and load them into the simulation environment. Create optional visual and auditory perceptual-encoding models in C++ and load them into the simulation environment. Create a file containing If-Then style production rules, that embody a hypothesized task performance strategy, and load it into the simulation environment. Set various simulation settings (how many trials to run, how often to update text output information, how often to update graphical output information, which EPIC sub-systems to trace during simulation, device parameters, and rule breakpoints) Once the simulation has been started and completed, the task device usually saves a representation of the simulated task's data on disk for later analysis. The device also may print rudimentary summary data to text output screen. EPIC includes routines to compute basic descriptive statistics and basic goodness-of-fit indicators such as correlation, slope, RMSE , and R^2. Of course, device programmers are free to write any additional statistics or goodness-of-fit routines they'd like. These routines can sometimes be in service of device logic (e.g., chaining device state if performance falls below some threshold), they are often used for simple post-simulation indications of whether a model is performing according to expectation (e.g., too many errors, or unexpectedly slow). More sophisticated analyses are typically done outside EPICapp using separate statistics software (e.g., R , SPSS , JASP , SAS , STATA , etc.). The biggest advantage of using EPICapp to run model simulations is its speed. Because all components (EPICLib, task devices, and perceptual encoders) are written in the highly efficient C++ language and compiled to machine code, EPICapp simulations are extremely fast. Another advantage is that because programming EPIC, devices, and encoders can occur within a single XCode project, it is easy to write and debug any of these components together using a single debugger context. E.g., one could run a simulation and set breakpoints within either EPIC, the device, or an encoder and then be able to inspect code and data across the entire project. EPICapp Challenges That Prompted EPICpy For those of us who use EPIC in our teaching and research may have encountered four key challenges: 1. C++ Language Expertise is Less Common There is no doubt that C++ is fast, and its choice for programming EPIC was obvious (in fact, it was a massive improvement on its previous implementation in Common Lisp ). However, the expertise required to program devices and encoders in C++ is not as common as it used to be. When EPICapp was created, C++ (or perhaps Java, a related skill) was the most common introductory programming language for Computer Science and Cognitive Science majors across the country. Now, this is quite rare, with Python being a significantly more common introductory language. This means that a researcher interested in EPIC modeling will need to acquire sufficient C++ skill to write device and encoder code. This also means that finding graduate students and other assistants with the programming skill to write device and encoder code will be more difficult than it once was. Although compiled C++ is blazingly fast to run, it is not easy or quick to read or write. Admittedly, simple devices and encoders won't pose too much trouble and may often require only moderate changes to existing example devices or encoders. However, more complex changes may require more serious programming skill. An EPIC modeling facility based on Python instead would lead to slower runtime execution, but much faster development time, trading computational efficiency for programmer efficiency. 2. Development Environment Required C++ is a compiled language and devices or encoders loaded into EPICapp must be compiled. This means that anyone who wishes to engage in EPIC modeling must set up a full EPIC programming environment so that edited code can be properly compiled. This is not an overly restrictive requirement. For example, setting up the development environment on MacOS isn't that difficult. However, the fact that devices and encoders must be compiled means that they are not particularly portable. Although there is only currently a MacOS version of EPICapp, if there were versions for other operating systems, devices and encoders would have to be compiled anew for each new OS target. In fact, this could even be a challenge across machines running MacOS if there were sufficiently significant changes to the OS. Soon, different versions will be needed to accommodate those using Apple computers with Intel-based chips and those using the new Apple M-series chips. If, on the other hand, devices and encoders were programmed with the Python language, the raw uncompiled code (just simple text files) could be loaded directly into the simulation without the need for explicit compilation. Thus, devices and encoders could be freely shared between EPIC modelers regardless of what type of machine they were using. 3. Statistical Abilities Limited EPIC does not natively contain much in the way of statistical routines. Frequently, after running a model, we compare the simulated output to observed human data. We may also want to compare it to output from other models, or pure mathematical functions. We often find ourselves doing complex data aggregation (e.g., changing 'tall' data arrangements to 'wide' ones) and statistical analyses (e.g., regression, analysis of variance, etc.). Furthermore, statistical analyses are often complemented by graphs and figures tha further clarify the correspondence between simulated EPIC data and observed human data. These analyses and figures are produced by importing data from the model run into a separate statistical analysis tool where custom routines are written. This has worked well, but sharing modeling work with others (students, colleagues, etc.) now means setting up both the EPIC development environment, and a corresponding statistical analysis framework. An alternative to requiring external statistical analysis tools would be to use the device to create sufficient statistical routines. Thus, the statistical analyses are no more difficult to share than the device itself. This works, but note that many statistical tests are highly non-trivial to write. E.g., writing your own code for a generalized mixed-model repeated-measures anova is extraordinarily difficulty and would require both advanced programming ability and significant statistical prowess. It is not clear how one would producing graphs and figures directly from the device code without significant edits to the EPICapp GUI code itself (possible, but non-trivial). If, devices were programmed in Python, modelers could easily take advantages of a rich set of easy to use free external modules and packages that allow statistical analyses ranging from the very simple to the very complex (e.g., scipy.stats and statsmodels. Furthermore, a similarly impressive set of powerful graphing facilities would be available. Most of these modules (many based on the popular Matplotlib library, e.g., Seaborn ) would allow the display of graphs. 4. Lack of Cross-Platform Versions of EPICapp Although EPIC itself could be adapted by a skilled programmer to compile bespoke versions for other operating systems, a fully working and runnable EPIC-only code base has yet to be made public. However, EPICapp, the tool one uses to actually manage and run EPIC simulations is only available for MacOS. Not only is the EPICapp GUI written in mac-specific Objective-C targeting Apple's own windowing facilities, but the development project setup and configuration are not publicly specified. Thus, in order to convert the EPICapp development environment to another operating system, one would have to completely deconstruct the development environment requirements, and then re-write the entire GUI and simulation logic in a new (hopefully cross-platform) GUI framework. This conversion could, of course, be done, but it is not trivial. Because this has yet to be done, currently the only available version of EPICapp is for MacOS. How EPICpy Addresses EPICapp Challenges The previous section lays out 4 main challenges that potential EPIC modelers face: Reliance on the C++ programming language limits programmer pool. Reliance on compiled code requires both development environment setup and limits device and encoder sharing between platforms. Statistical facilities limited, no graphing available. Only runs on system running MacOS. EPICpy was created to address these issues. EPICpy uses Python rather than C++ First, it should be pointed out that EPICpy is not completely written in Python. EPICLib itself, that is, the cognitive architecture and various routines to allow creation and management of simulations using EPIC's subsystems, is still written in C++ and compiled into an importable module for Python on Macos, Linux, and Windows. Everything else is programmed in Python. This includes task devices and perceptual encoders, which are programmed in Python and are stored and loaded into EPICpy as raw Python code. This means that no compilation is required; An EPICpy device written on one operating system will work exactly the same on another operating system. Most EPIC modeling is some combination of work on devices, encoders, and rule files. In EPICpy, these are all just text files. EPICpy requires no device/encoder compilation Because EPICpy devices and encoders are now just text-based Python code files, it means that the majority of EPIC modeling work can now be done with a code/text editor, no C++ or Python development environment is required. Just update your device in a text editor, have EPICpy re-load it, and press run to see your changes (or error messages, if something goes awry). In a sense, EPICpy itself becomes your development environment. Because Python code is relatively easy to read and write, it will be much easier to find students, assistants and collaborators who have sufficient skill to create or edit devices and encoders. EPICpy GUI written in Python The EPICpy GUI application itself is written in Python. This means that if one desires to actually alter the operation of the EPICpy interface, they are free to do so. However, this would require setting up the EPICpy development environment on your computer. Running such an updated version of EPICpy would be easy, but compiling your changes back into a cross-platform executable will take further steps and knowledge. If you want to do this, see the appropriate \" EPICpy Development Environment \" section on the Installing EPICpy page. EPICpy allows use of Python statistical and graphing Python has a rich set of modules and packages for statistical analysis and graphing. Because EPICpy includes the Pingouin Statistical Package , device programmers have access to the following facilities (see Pingouin docs for other imported packages): pingouin statsmodels pandas pandas-flavor numpy scipy matplotlib seaborn Access to these packages in device code is enabled through standard Python import statements. Display of statistical analyses and graphs is enabled by using the stats_write() method which can accept text, pandas dataframes and other objects, as well as figures and other matplotlib-based objects. Although we don't recommend programming full data analyses at the end of each simulation run, this statistical facility will allow most conceivable post-run analyses a modeler might need. EPICpy is Cross Platform EPICpy was developed by Travis L. Seymour, PhD on the Ubuntu Linux operating system (as well as variants such as Linux Mint and Pop!_OS ). To facilitate the installation and use of EPICpy across platforms, we are using the PipX system for automatically running Python applications in isolated environments. After users install PipX itself, it's a single command across operating systems to install EPICpy. However, because EPICLib itself is compiled, there are some limitations: On Linux and Macos, Python 3.10.?? must be installed, and on Windows 10 and 11, Python 3.9.?? must be installed. Once PipX and the needed version of Python are installed, EPICpy should install and run on either of these three operating systems. For detailed installation instructions, please see the corresponding section on the Installing EPICpy page. Thus, EPICpy can be used on Linux, MacOS (Intel and Apple CPUs), and Windows based computers. Other New Features in EPICpy (in no particular order) * Dark/Light Mode Toggle * Background Images For View Windows (not useful for modeling, but great for talks and demonstrations) * Text and Stats Output Window Content Exporting (as text, html, markdown and odf) * Device load triggers automatic load of last ruleset used with that device * One-Button Reinstatement of Previous Modeling Session (i.e., device, rules, encoders) * Automatically Remembers and Reinstates Dimensions of all Dialog Windows. * Automatically Reinstates GUI Settings and Parameters For Each Device (including all window positions and sizes) * Ability to Load and Run Multiple Rulesets Sequentially * Ability to Add Parameters to the Device Condition String to enable parametric model runs (e.g., 10 [Easy|Hard] would run 10 trials of the Easy condition and then 10 trial of the Hard condition.) * Adjustable Application Font Size * Adjustable Detail for Sound and Speech Object Display (e.g., determine which attributes of the Speech_word class to show on the perceptual output window) * EPICLib is now fixed to the 2016 release. * Access to Some Device Configuration from the GUI (e.g., device programmer might expose toggle for showing debug information, or altering device operation) * Added Window-Management Facilities (e.g., Double-clicking output window brings all windows to front, Minimize all windows, Show all windows, Restore Default Layout) * It is Now Possible to UN load perceptual encoders. Unloaded encoders will not be re-loaded during subsequent session reloads. Jan 25 2025 5:27pm","title":"Home"},{"location":"#welcome-to-the-epicpy-docs","text":"You can find EPICpy related content at one of these locations: Overview Detailed Documentation @ GitHub Pages (You're Reading It Now!) EPICpy Article Development Environment @ Github If you want to jump directly to a step-by-step example of how you use EPICpy to do EPIC modeling (with an admittedly toy-example task), see the Example page. If you are looking for a quick test to make sure EPICpy is working correctly on your computer, go to the An EPIC Model of the Human Data section on the Example page and follow the directions under Initial Model Process Diagram .","title":"Welcome to the EPICpy Docs"},{"location":"#what-these-docs-are","text":"This is the documentation for EPICpy , a tool for simulating human performance tasks using EPIC (a computational cognitive architecture created by David Kieras and David Meyer) . For a brief overview of EPIC and EPICpy, see the sections below.","title":"What These Docs ARE"},{"location":"#what-these-docs-are-not","text":"This documentation is not a detailed introduction to EPIC, device programming, perceptual encoder programming, or production rule programming . To learn more about EPIC and how it works, please see the documents in the Epic Resources section (in particular, the first three).","title":"What These Docs ARE NOT"},{"location":"#what-is-epic","text":"EPIC Schematic (Meyer & Kieras 1997a) EPIC is a computational cognitive architecture that specifies a theory of human performance and a facility to create individual task models constrained by that theory. EPIC compacts decades of psychological theory and findings into a rich set of processors, including those that model sensory, perceptual, motor, cognitive, and memory subsystems of the human mind. EPIC is particularly suited to stimulus-response type human performance tasks including those involving multitasking and task-switching. The goal of creating model simulations in EPIC is typically to either verify your understanding of human performance in some task for which you already have human data, or to predict human performance in some speculative task. Either way, this typically involves comparing simulated data from EPIC (e.g., simulated response times, accuracy rates, eye movements, etc.) to human data using a similar task. Over many publications (see \"Various Published Uses of EPIC\" within the EPIC Resources section , Kiears, Meyer, and their colleagues have demonstrated that if you 1) create a simulated task within EPIC (called a \"device\") sufficiently close to the corresponding human task and 2) create a simulated task strategy (embodied within a set of \"production rules\" and model configurations) sufficiently close to the strategy humans used when performing the task, then EPIC will produce a set of simulated performance data that follows very close (often nearly identical) to the observed human data.","title":"What is EPIC"},{"location":"#what-is-epicapp","text":"(the original tool available for EPIC modeling) The EPIC architecture is a C++ code library (called EPIClib ) capable of simulating human performance according to a particular theory of human performance using a massive set of defaults chosen carefully from various past literatures (with a few guesses where none exists). Although most EPIC modellers only use EPICLib, It is possible to alter the EPIC architecture itself to add new processors, change how various components operate, and so on. This requires an advanced level of C++ programming skill (including mastery of various design patterns, and abstraction). EPICLib module is not generally usable on its own and so David Kieras created EPICapp. EPICapp is a Graphical User Interface (GUI) based application that serves as a controller for EPIC simulations, as well as offering a set of views for visualizing simulation progress. It allows researchers to: Create simulated task devices in C++ and load them into the simulation environment. Create optional visual and auditory perceptual-encoding models in C++ and load them into the simulation environment. Create a file containing If-Then style production rules, that embody a hypothesized task performance strategy, and load it into the simulation environment. Set various simulation settings (how many trials to run, how often to update text output information, how often to update graphical output information, which EPIC sub-systems to trace during simulation, device parameters, and rule breakpoints) Once the simulation has been started and completed, the task device usually saves a representation of the simulated task's data on disk for later analysis. The device also may print rudimentary summary data to text output screen. EPIC includes routines to compute basic descriptive statistics and basic goodness-of-fit indicators such as correlation, slope, RMSE , and R^2. Of course, device programmers are free to write any additional statistics or goodness-of-fit routines they'd like. These routines can sometimes be in service of device logic (e.g., chaining device state if performance falls below some threshold), they are often used for simple post-simulation indications of whether a model is performing according to expectation (e.g., too many errors, or unexpectedly slow). More sophisticated analyses are typically done outside EPICapp using separate statistics software (e.g., R , SPSS , JASP , SAS , STATA , etc.). The biggest advantage of using EPICapp to run model simulations is its speed. Because all components (EPICLib, task devices, and perceptual encoders) are written in the highly efficient C++ language and compiled to machine code, EPICapp simulations are extremely fast. Another advantage is that because programming EPIC, devices, and encoders can occur within a single XCode project, it is easy to write and debug any of these components together using a single debugger context. E.g., one could run a simulation and set breakpoints within either EPIC, the device, or an encoder and then be able to inspect code and data across the entire project.","title":"What is EPICapp"},{"location":"#epicapp-challenges-that-prompted-epicpy","text":"For those of us who use EPIC in our teaching and research may have encountered four key challenges:","title":"EPICapp Challenges That Prompted EPICpy"},{"location":"#1-c-language-expertise-is-less-common","text":"There is no doubt that C++ is fast, and its choice for programming EPIC was obvious (in fact, it was a massive improvement on its previous implementation in Common Lisp ). However, the expertise required to program devices and encoders in C++ is not as common as it used to be. When EPICapp was created, C++ (or perhaps Java, a related skill) was the most common introductory programming language for Computer Science and Cognitive Science majors across the country. Now, this is quite rare, with Python being a significantly more common introductory language. This means that a researcher interested in EPIC modeling will need to acquire sufficient C++ skill to write device and encoder code. This also means that finding graduate students and other assistants with the programming skill to write device and encoder code will be more difficult than it once was. Although compiled C++ is blazingly fast to run, it is not easy or quick to read or write. Admittedly, simple devices and encoders won't pose too much trouble and may often require only moderate changes to existing example devices or encoders. However, more complex changes may require more serious programming skill. An EPIC modeling facility based on Python instead would lead to slower runtime execution, but much faster development time, trading computational efficiency for programmer efficiency.","title":"1. C++ Language Expertise is Less Common"},{"location":"#2-development-environment-required","text":"C++ is a compiled language and devices or encoders loaded into EPICapp must be compiled. This means that anyone who wishes to engage in EPIC modeling must set up a full EPIC programming environment so that edited code can be properly compiled. This is not an overly restrictive requirement. For example, setting up the development environment on MacOS isn't that difficult. However, the fact that devices and encoders must be compiled means that they are not particularly portable. Although there is only currently a MacOS version of EPICapp, if there were versions for other operating systems, devices and encoders would have to be compiled anew for each new OS target. In fact, this could even be a challenge across machines running MacOS if there were sufficiently significant changes to the OS. Soon, different versions will be needed to accommodate those using Apple computers with Intel-based chips and those using the new Apple M-series chips. If, on the other hand, devices and encoders were programmed with the Python language, the raw uncompiled code (just simple text files) could be loaded directly into the simulation without the need for explicit compilation. Thus, devices and encoders could be freely shared between EPIC modelers regardless of what type of machine they were using.","title":"2. Development Environment Required"},{"location":"#3-statistical-abilities-limited","text":"EPIC does not natively contain much in the way of statistical routines. Frequently, after running a model, we compare the simulated output to observed human data. We may also want to compare it to output from other models, or pure mathematical functions. We often find ourselves doing complex data aggregation (e.g., changing 'tall' data arrangements to 'wide' ones) and statistical analyses (e.g., regression, analysis of variance, etc.). Furthermore, statistical analyses are often complemented by graphs and figures tha further clarify the correspondence between simulated EPIC data and observed human data. These analyses and figures are produced by importing data from the model run into a separate statistical analysis tool where custom routines are written. This has worked well, but sharing modeling work with others (students, colleagues, etc.) now means setting up both the EPIC development environment, and a corresponding statistical analysis framework. An alternative to requiring external statistical analysis tools would be to use the device to create sufficient statistical routines. Thus, the statistical analyses are no more difficult to share than the device itself. This works, but note that many statistical tests are highly non-trivial to write. E.g., writing your own code for a generalized mixed-model repeated-measures anova is extraordinarily difficulty and would require both advanced programming ability and significant statistical prowess. It is not clear how one would producing graphs and figures directly from the device code without significant edits to the EPICapp GUI code itself (possible, but non-trivial). If, devices were programmed in Python, modelers could easily take advantages of a rich set of easy to use free external modules and packages that allow statistical analyses ranging from the very simple to the very complex (e.g., scipy.stats and statsmodels. Furthermore, a similarly impressive set of powerful graphing facilities would be available. Most of these modules (many based on the popular Matplotlib library, e.g., Seaborn ) would allow the display of graphs.","title":"3. Statistical Abilities Limited"},{"location":"#4-lack-of-cross-platform-versions-of-epicapp","text":"Although EPIC itself could be adapted by a skilled programmer to compile bespoke versions for other operating systems, a fully working and runnable EPIC-only code base has yet to be made public. However, EPICapp, the tool one uses to actually manage and run EPIC simulations is only available for MacOS. Not only is the EPICapp GUI written in mac-specific Objective-C targeting Apple's own windowing facilities, but the development project setup and configuration are not publicly specified. Thus, in order to convert the EPICapp development environment to another operating system, one would have to completely deconstruct the development environment requirements, and then re-write the entire GUI and simulation logic in a new (hopefully cross-platform) GUI framework. This conversion could, of course, be done, but it is not trivial. Because this has yet to be done, currently the only available version of EPICapp is for MacOS.","title":"4. Lack of Cross-Platform Versions of EPICapp"},{"location":"#how-epicpy-addresses-epicapp-challenges","text":"The previous section lays out 4 main challenges that potential EPIC modelers face: Reliance on the C++ programming language limits programmer pool. Reliance on compiled code requires both development environment setup and limits device and encoder sharing between platforms. Statistical facilities limited, no graphing available. Only runs on system running MacOS. EPICpy was created to address these issues.","title":"How EPICpy Addresses EPICapp Challenges"},{"location":"#epicpy-uses-python-rather-than-c","text":"First, it should be pointed out that EPICpy is not completely written in Python. EPICLib itself, that is, the cognitive architecture and various routines to allow creation and management of simulations using EPIC's subsystems, is still written in C++ and compiled into an importable module for Python on Macos, Linux, and Windows. Everything else is programmed in Python. This includes task devices and perceptual encoders, which are programmed in Python and are stored and loaded into EPICpy as raw Python code. This means that no compilation is required; An EPICpy device written on one operating system will work exactly the same on another operating system. Most EPIC modeling is some combination of work on devices, encoders, and rule files. In EPICpy, these are all just text files.","title":"EPICpy uses Python rather than C++"},{"location":"#epicpy-requires-no-deviceencoder-compilation","text":"Because EPICpy devices and encoders are now just text-based Python code files, it means that the majority of EPIC modeling work can now be done with a code/text editor, no C++ or Python development environment is required. Just update your device in a text editor, have EPICpy re-load it, and press run to see your changes (or error messages, if something goes awry). In a sense, EPICpy itself becomes your development environment. Because Python code is relatively easy to read and write, it will be much easier to find students, assistants and collaborators who have sufficient skill to create or edit devices and encoders.","title":"EPICpy requires no device/encoder compilation"},{"location":"#epicpy-gui-written-in-python","text":"The EPICpy GUI application itself is written in Python. This means that if one desires to actually alter the operation of the EPICpy interface, they are free to do so. However, this would require setting up the EPICpy development environment on your computer. Running such an updated version of EPICpy would be easy, but compiling your changes back into a cross-platform executable will take further steps and knowledge. If you want to do this, see the appropriate \" EPICpy Development Environment \" section on the Installing EPICpy page.","title":"EPICpy GUI written in Python"},{"location":"#epicpy-allows-use-of-python-statistical-and-graphing","text":"Python has a rich set of modules and packages for statistical analysis and graphing. Because EPICpy includes the Pingouin Statistical Package , device programmers have access to the following facilities (see Pingouin docs for other imported packages): pingouin statsmodels pandas pandas-flavor numpy scipy matplotlib seaborn Access to these packages in device code is enabled through standard Python import statements. Display of statistical analyses and graphs is enabled by using the stats_write() method which can accept text, pandas dataframes and other objects, as well as figures and other matplotlib-based objects. Although we don't recommend programming full data analyses at the end of each simulation run, this statistical facility will allow most conceivable post-run analyses a modeler might need.","title":"EPICpy allows use of Python statistical and graphing"},{"location":"#epicpy-is-cross-platform","text":"EPICpy was developed by Travis L. Seymour, PhD on the Ubuntu Linux operating system (as well as variants such as Linux Mint and Pop!_OS ). To facilitate the installation and use of EPICpy across platforms, we are using the PipX system for automatically running Python applications in isolated environments. After users install PipX itself, it's a single command across operating systems to install EPICpy. However, because EPICLib itself is compiled, there are some limitations: On Linux and Macos, Python 3.10.?? must be installed, and on Windows 10 and 11, Python 3.9.?? must be installed. Once PipX and the needed version of Python are installed, EPICpy should install and run on either of these three operating systems. For detailed installation instructions, please see the corresponding section on the Installing EPICpy page. Thus, EPICpy can be used on Linux, MacOS (Intel and Apple CPUs), and Windows based computers.","title":"EPICpy is Cross Platform"},{"location":"#other-new-features-in-epicpy","text":"(in no particular order) * Dark/Light Mode Toggle * Background Images For View Windows (not useful for modeling, but great for talks and demonstrations) * Text and Stats Output Window Content Exporting (as text, html, markdown and odf) * Device load triggers automatic load of last ruleset used with that device * One-Button Reinstatement of Previous Modeling Session (i.e., device, rules, encoders) * Automatically Remembers and Reinstates Dimensions of all Dialog Windows. * Automatically Reinstates GUI Settings and Parameters For Each Device (including all window positions and sizes) * Ability to Load and Run Multiple Rulesets Sequentially * Ability to Add Parameters to the Device Condition String to enable parametric model runs (e.g., 10 [Easy|Hard] would run 10 trials of the Easy condition and then 10 trial of the Hard condition.) * Adjustable Application Font Size * Adjustable Detail for Sound and Speech Object Display (e.g., determine which attributes of the Speech_word class to show on the perceptual output window) * EPICLib is now fixed to the 2016 release. * Access to Some Device Configuration from the GUI (e.g., device programmer might expose toggle for showing debug information, or altering device operation) * Added Window-Management Facilities (e.g., Double-clicking output window brings all windows to front, Minimize all windows, Show all windows, Restore Default Layout) * It is Now Possible to UN load perceptual encoders. Unloaded encoders will not be re-loaded during subsequent session reloads. Jan 25 2025 5:27pm","title":"Other New Features in EPICpy"},{"location":"epicinterface_options/","text":"EPICpy Interface Options, Settings, and Commands When you first run EPICpy, the graphical user interface (GUI) should look like this: File Menu Load Device : Presents a file dialog that allows you to select and load an EPICpy \"Device File\". Device files define a particular virtual task and are coded in Python. Compile Rules : Presents a file dialog that allows you to select and load an EPICpy \"Production Rule\" text file. Production Rules are written in IF-THEN style according to the Parsimonious Production Rule System (Covrigaru & Kieras 1995) . The verb \"Compile\" indicates that after loading this file, EPIC will attempt to evaluate the rules and use them to construct a rule network. Any errors in the rule syntax will be noted in the Normal Output Window. Recompile Rules : Compiles the most recently production rule file to have been successfully compiled. Export Normal/Trace/Stats Output : Opens a file-save dialog allowing to the contents of the Normal-, Trace-, or Stats-Window to be stored as a file on disk. Find Menu Find : Launches a dialog (also available by right-clicking the Normal Output Window) that allows simple search of text within the Normal Output Window. Settings Menu Trace Settings Dialog This dialog allows detailed tracing of various EPIC processors' information during simulation. Relevant output is printed to the Trace Output Window. Display Controls Dialog PPS Memory Contents : Prints EPIC Working Memory (WM) contents to the Normal Output Window during simulation runs. PPS Run Messages : Prints information about which Production Rules fire during simulation runs, as well as the relevant WM elements that match rules that fired (called \"rule bindings\"). PPS Compiler Messages : Prints extra diagnostic information when (re)compiling production rules. Spatial scale pixels/degree : Magnification factor for view windows. Larger values zoom in, smaller values zoom out. Show Model Parameter Before Each Run : Does as the name suggests, can be useful when saving Normal Output Window content to have an indication of the corresponding EPIC parameterization. If you are new to EPIC modelling, you should study this output -- the defaults may or may not be relevant for your task, although care should be taken before changing EPIC parameters without sufficient cause. Allow device to draw view underlay images : It is possible for devices to specify images the draw underneath normal Visual- or Auditory View content. This toggle allows such drawing to be enabled/disabled without altering the device code. Images need to be placed in a folder next to the device called images . Images will only be shown if this setting is ON and the task device explicitly calls for them to be drawn. Compiler Details : Currently does nothing. Run Details : Prints detailed information about the operation of PPS during simulation runs. Not often useful for most modellers, but can be useful for investigating why complex rules are or are not firing. Center Dot : Toggles the drawing of a small dot on Visual Views to indicate where the center of the view is as other objects move around. Calibration Grid : Toggles the drawing of a grid overlay across each of the View windows (may slow down simulation on some systems). Rule Break Settings Dialog This dialog shows a toggle for each rule in the current ruleset. Checking a rule will cause the simulation to halt when it fires. The entire rule-break facility can be toggled with the topmost checkbox without altering which individual rules are selected. Logging Dialog Although it is possible to save Normal and Trace Output content after a run, this dialog can be used to set files to automatically receive content for these windows dynamically during the simulation run (of course, this constant writing to disk may slow down the simulation on some systems). Device Options Dialog This dialog will present 0 or more checkboxes that allow toggling features of the current task device without altering the corresponding code. Note that exposing options for this menu is optional and some devices may not expose any. EPICLib Settings Dialog NOTE: This dialog has been disabled for now -- currently, only the 06/08/2016 version of EPICLib is available. Sound Text Settings Dialog Sound and Speech objects displayed on the Auditory Views can be adorned with several pieces of information. Note that sounds always show the Timbre and Speech always show the Content. Font Settings Dialog Although the font used by EPICpy is set, this dialog allows you to set the font size used. Text Editor Settings Dialog Although the font used by EPICpy is set, this dialog allows you to set the font size used. Running/Managing Simulations Once you have your device and rules loaded (an perhaps an encoder or 2), you are ready to run a simulation. The Run->Run_Settings menu allows you to view and alter the parameters for running your simulation: In most instances, you will want to enable the Run Command Duration Run Until Done , the Display Refresh After Each Step and the Text Refresh After Every 2 steps , with a Real Time Per Step of 0 msecs(s) . If you are starting a new run (updated device, ruleset, etc.), you may want to clear out the data using the [Delete Current Device Datafile] button and then press [ OK ]. At this point, you are ready to choose the Run->Run menu option to start your simulation. The lists below explain the other options available on the Run Settings dialog. Run Command Duration Run Command Duration allows you to specify the stopping rule for the simulation: Run For : Specifies a number of simulated milliseconds or a steps to run. You can specify this value in steps (third row from the top) or in milliseconds (first row). Steps are 50 ms in duration, running for 50 ms is equivalent to running 1 step. No matter where you are in the simulation (other than the end), this will run for X more ms. Think of this as asking EPIC to run for a specified simulated time period. Run Until : Specifies a simulated timestep within the simulation -- when this point has been reached, the simulation halts. If you press Run->Run while already at or past this point, nothing will be done. Think of this as asking EPIC to run until a particular simulated temporal milestone. Display Refresh As the simulation runs, EPIC will draw any visual or auditory information being represented on the device and in the sensory and perceptual systems on the Physical, Sensory, and Perceptual View windows, respectively. Although updates to these windows are being done quickly, these drawing routines and the information exchange that fuel them can slow down the simulation. In general, when developing a model, it is common to enable display refresh so that you can visually monitor the view windows as the simulation progresses. However, when producing large simulated runs, it will be much faster to disable this graphical output. Here are the options: After Each Step : This is essentially the normal mode of operation whereby the contents of the device display and EPIC's visual and perceptual processors are depicted in the view windows dynamically as the simulation progresses. This mode is significantly slower than choosing None During Run . None During Run : In this mode, view window updates are completely disabled. When the simulation ends, you may see something drawn on the view windows, but only if there were objects represented in the corresponding processors when the simulation ended. Text Refresh As the simulation runs, EPIC will produce a decent amount of text -- primarily consisting of a dump of EPIC's Working Memory (WM) after each 50 ms step. These settings allow you to manage how often this text is actually printed to the Normal Output and Trace windows during the simulation. The reason you might care is that printing text slows down the simulation somewhat. In general, when developing a model, it is common to enable text refresh so that you can visually monitor the output as the simulation progresses. However, when producing large simulated runs, it will be much faster to disable this text refresh. Here are the options: Continuously : In this mode, EPICpy will essentially print text to the Normal Output and Trace windows as soon as it is generated,thus you will see text flying by continuously as the simulation progresses. The simulation will be significantly slower in this mode. After Each X Steps : In this mode, EPICpy will cache printed text and only print it to the Normal Output after the specified number of steps. This mode is much faster than the Continuously mode. None During Run : In this mode, EPICpy will cache printed text and only print it to the Normal Output and Trace windows after the entire simulation run has finished. This mode will speed up the simulation dramatically compared to the Continuously mode. Note that simulations that generate a lot of text (either long runs, and/or runs what a great deal of WM content) may take a few moments to render all the cached text after the simulation finishes.","title":"EPICpy GUI Options"},{"location":"epicinterface_options/#epicpy-interface-options-settings-and-commands","text":"When you first run EPICpy, the graphical user interface (GUI) should look like this:","title":"EPICpy Interface Options, Settings, and Commands"},{"location":"epicinterface_options/#file-menu","text":"Load Device : Presents a file dialog that allows you to select and load an EPICpy \"Device File\". Device files define a particular virtual task and are coded in Python. Compile Rules : Presents a file dialog that allows you to select and load an EPICpy \"Production Rule\" text file. Production Rules are written in IF-THEN style according to the Parsimonious Production Rule System (Covrigaru & Kieras 1995) . The verb \"Compile\" indicates that after loading this file, EPIC will attempt to evaluate the rules and use them to construct a rule network. Any errors in the rule syntax will be noted in the Normal Output Window. Recompile Rules : Compiles the most recently production rule file to have been successfully compiled. Export Normal/Trace/Stats Output : Opens a file-save dialog allowing to the contents of the Normal-, Trace-, or Stats-Window to be stored as a file on disk.","title":"File Menu"},{"location":"epicinterface_options/#find-menu","text":"Find : Launches a dialog (also available by right-clicking the Normal Output Window) that allows simple search of text within the Normal Output Window.","title":"Find Menu"},{"location":"epicinterface_options/#settings-menu","text":"","title":"Settings Menu"},{"location":"epicinterface_options/#trace-settings-dialog","text":"This dialog allows detailed tracing of various EPIC processors' information during simulation. Relevant output is printed to the Trace Output Window.","title":"Trace Settings Dialog"},{"location":"epicinterface_options/#display-controls-dialog","text":"PPS Memory Contents : Prints EPIC Working Memory (WM) contents to the Normal Output Window during simulation runs. PPS Run Messages : Prints information about which Production Rules fire during simulation runs, as well as the relevant WM elements that match rules that fired (called \"rule bindings\"). PPS Compiler Messages : Prints extra diagnostic information when (re)compiling production rules. Spatial scale pixels/degree : Magnification factor for view windows. Larger values zoom in, smaller values zoom out. Show Model Parameter Before Each Run : Does as the name suggests, can be useful when saving Normal Output Window content to have an indication of the corresponding EPIC parameterization. If you are new to EPIC modelling, you should study this output -- the defaults may or may not be relevant for your task, although care should be taken before changing EPIC parameters without sufficient cause. Allow device to draw view underlay images : It is possible for devices to specify images the draw underneath normal Visual- or Auditory View content. This toggle allows such drawing to be enabled/disabled without altering the device code. Images need to be placed in a folder next to the device called images . Images will only be shown if this setting is ON and the task device explicitly calls for them to be drawn. Compiler Details : Currently does nothing. Run Details : Prints detailed information about the operation of PPS during simulation runs. Not often useful for most modellers, but can be useful for investigating why complex rules are or are not firing. Center Dot : Toggles the drawing of a small dot on Visual Views to indicate where the center of the view is as other objects move around. Calibration Grid : Toggles the drawing of a grid overlay across each of the View windows (may slow down simulation on some systems).","title":"Display Controls Dialog"},{"location":"epicinterface_options/#rule-break-settings-dialog","text":"This dialog shows a toggle for each rule in the current ruleset. Checking a rule will cause the simulation to halt when it fires. The entire rule-break facility can be toggled with the topmost checkbox without altering which individual rules are selected.","title":"Rule Break Settings Dialog"},{"location":"epicinterface_options/#logging-dialog","text":"Although it is possible to save Normal and Trace Output content after a run, this dialog can be used to set files to automatically receive content for these windows dynamically during the simulation run (of course, this constant writing to disk may slow down the simulation on some systems).","title":"Logging Dialog"},{"location":"epicinterface_options/#device-options-dialog","text":"This dialog will present 0 or more checkboxes that allow toggling features of the current task device without altering the corresponding code. Note that exposing options for this menu is optional and some devices may not expose any.","title":"Device Options Dialog"},{"location":"epicinterface_options/#epiclib-settings-dialog","text":"NOTE: This dialog has been disabled for now -- currently, only the 06/08/2016 version of EPICLib is available.","title":"EPICLib Settings Dialog"},{"location":"epicinterface_options/#sound-text-settings-dialog","text":"Sound and Speech objects displayed on the Auditory Views can be adorned with several pieces of information. Note that sounds always show the Timbre and Speech always show the Content.","title":"Sound Text Settings Dialog"},{"location":"epicinterface_options/#font-settings-dialog","text":"Although the font used by EPICpy is set, this dialog allows you to set the font size used.","title":"Font Settings Dialog"},{"location":"epicinterface_options/#text-editor-settings-dialog","text":"Although the font used by EPICpy is set, this dialog allows you to set the font size used.","title":"Text Editor Settings Dialog"},{"location":"epicinterface_options/#runningmanaging-simulations","text":"Once you have your device and rules loaded (an perhaps an encoder or 2), you are ready to run a simulation. The Run->Run_Settings menu allows you to view and alter the parameters for running your simulation: In most instances, you will want to enable the Run Command Duration Run Until Done , the Display Refresh After Each Step and the Text Refresh After Every 2 steps , with a Real Time Per Step of 0 msecs(s) . If you are starting a new run (updated device, ruleset, etc.), you may want to clear out the data using the [Delete Current Device Datafile] button and then press [ OK ]. At this point, you are ready to choose the Run->Run menu option to start your simulation. The lists below explain the other options available on the Run Settings dialog. Run Command Duration Run Command Duration allows you to specify the stopping rule for the simulation: Run For : Specifies a number of simulated milliseconds or a steps to run. You can specify this value in steps (third row from the top) or in milliseconds (first row). Steps are 50 ms in duration, running for 50 ms is equivalent to running 1 step. No matter where you are in the simulation (other than the end), this will run for X more ms. Think of this as asking EPIC to run for a specified simulated time period. Run Until : Specifies a simulated timestep within the simulation -- when this point has been reached, the simulation halts. If you press Run->Run while already at or past this point, nothing will be done. Think of this as asking EPIC to run until a particular simulated temporal milestone. Display Refresh As the simulation runs, EPIC will draw any visual or auditory information being represented on the device and in the sensory and perceptual systems on the Physical, Sensory, and Perceptual View windows, respectively. Although updates to these windows are being done quickly, these drawing routines and the information exchange that fuel them can slow down the simulation. In general, when developing a model, it is common to enable display refresh so that you can visually monitor the view windows as the simulation progresses. However, when producing large simulated runs, it will be much faster to disable this graphical output. Here are the options: After Each Step : This is essentially the normal mode of operation whereby the contents of the device display and EPIC's visual and perceptual processors are depicted in the view windows dynamically as the simulation progresses. This mode is significantly slower than choosing None During Run . None During Run : In this mode, view window updates are completely disabled. When the simulation ends, you may see something drawn on the view windows, but only if there were objects represented in the corresponding processors when the simulation ended. Text Refresh As the simulation runs, EPIC will produce a decent amount of text -- primarily consisting of a dump of EPIC's Working Memory (WM) after each 50 ms step. These settings allow you to manage how often this text is actually printed to the Normal Output and Trace windows during the simulation. The reason you might care is that printing text slows down the simulation somewhat. In general, when developing a model, it is common to enable text refresh so that you can visually monitor the output as the simulation progresses. However, when producing large simulated runs, it will be much faster to disable this text refresh. Here are the options: Continuously : In this mode, EPICpy will essentially print text to the Normal Output and Trace windows as soon as it is generated,thus you will see text flying by continuously as the simulation progresses. The simulation will be significantly slower in this mode. After Each X Steps : In this mode, EPICpy will cache printed text and only print it to the Normal Output after the specified number of steps. This mode is much faster than the Continuously mode. None During Run : In this mode, EPICpy will cache printed text and only print it to the Normal Output and Trace windows after the entire simulation run has finished. This mode will speed up the simulation dramatically compared to the Continuously mode. Note that simulations that generate a lot of text (either long runs, and/or runs what a great deal of WM content) may take a few moments to render all the cached text after the simulation finishes.","title":"Running/Managing Simulations"},{"location":"epicinterface_windows/","text":"EPICpy Interface When you first run EPICpy, the graphical user interface (GUI) should look like this: Model View Windows During simulations, EPICpy allows the modeler to monitor content being represented by 3 key processors within the EPIC architecture; The Physical View, the Sensory View, and the Perceptual View. Visual Views During the Williams Task For example, the figure above depicts a snapshot from a simulation of the paradigm from Williams (1967) . On each trial, a large assortment of objects with various colors, shapes, and sizes are displayed in a grid configuration. Prior to this display, participants are asked to locate a particular object (e.g., medium, plus, green) and respond with a button press a soon as they have found it. The Physical View : The Physical View shows all visual information currently being represented on the display of the simulated device. The first panel of the figure above shows the Visual Physical View of the Williams Task. Notice that the current gaze position of EPIC is represented by a set of concentric circles. The center-most point of these circles represents the location of EPIC's current gaze point. The small filled grey circle extends to 1 degree of visual angle (DVA) around this gaze-point. The larger unfilled circle extends to 7.6 DVA. Modellers can use these circles to tell where EPIC is currently \"looking\". Like a computer monitor, when visual information is drawn on the device's virtual monitor, it is visible immediately on the Visual Physical View. Similarly, when information is removed from the simulated monitor, it is removed immediately from the Visual Physical View. The Sensory View : The second panel of the figure above shows the Visual Sensory View of the Williams Task. The Sensory View allows the Modeller to monitor which subset of the information displayed on the Visual Physical View is currently being represented by EPIC's ocular system. There is a delay between the time information appears on the device and when it can be represented by the eye, and so information on the Visual Sensory View will lag that on the Visual Physical View. This is also true for information removed from the device, there will be a short delay before it is no longer supported by the visual system. Note that in this snapshot of the Williams Task, EPIC only has complete information (shape, color, and text) of the shape directly in front of its fovea, partial information (e.g., shape) out to about 7 or so degrees, and beyond that it represents no information. Once the eye moves to a new location, and after appropriate psychological delays, it will only represent ocular information available at that location. Exactly what EPIC can see from a particular gaze location may be affected by the specific type and parameterization of any availability functions currently defined in the rules. The Perceptual View : The third panel of the figure above shows the Visual Perceptual View of the Williams Task. Just as EPIC's sensory system can only represent information available on the virtual display, EPIC's perceptual system can only represent information previously represented by the sensory system. The Visual Perceptual View does not represent the entire contents of Visual Perception (for that one would have to view the Working Memory trace; see below). However, visual object properties such as location, shape, size, relative position, color, text, etc. are depicted here. Although both EPIC's sensory system and its perceptual system maintain information even after it is no longer supported on the device, perceptual information stays around for considerably longer. Because of this, the Visual Perceptual View will show information about many of the objects that have been recently foveated by EPIC's visual system. Williams Task In Action (Slowed Down Greatly) The video above shows a set of trials of the Williams Task. EPIC first reads each of the 3 words that indicate which shape is the current trial's target stimulus. Notice the temporal dynamics of the 3 view windows; information presented on the display shows up on the Sensory Vision window after a delay, and on the Perception Vision window after another delay. After information is removed from the virtual device, it then disappears from the Sensory Vision window, and then later from the Perceptual Vision window. Auditory Views During the Choice Task There are also 3 corresponding view windows for EPIC's auditory system. The Auditory Physical View depicts any audio played by the virtual device. These can be sounds from a variety of virtual sources, including speakers, headphones, human speakers, nature, etc., and are represented as either a sound or speech within EPIC. No matter the source, sounds played by the device are depicted for the modeller as text located somewhere on the 2D space of the Auditory Physical View window. This text contains the name of the sound, the timbre (sounds) or text content (speech), as well as several other pieces of information (e.g., loudness, pitch, etc.). The exact set of information displayed for auditory objects is configurable via the Settings\u27a1Sound Text Settings dialog. If all available information is enabled, the Auditory View Windows may become too congested to follow. In the default Choice Task device, each trial's visual fixation is accompanied by a computerized speech sound of the word \"Warning\". This is denoted by Speaker: Computer, Content: Warning, Kind: Speech . Each trial's stimulus is accompanied by a non-speech sound with a timbre dependent on the stimulus color (in this case, \"Beep\"). This is denoted by Timbre: Beep, Stream: Signal, Kind: Sound . This particular output for both speech and sound objects is a subset of what is available in the Settings\u27a1Sound Text Settings dialog. For example, this particular output is achieved with the following setting: Note that Timbre and Content are grayed out because they cannot be disabled for sounds and speech, respectively. Normal Output Window Then Normal Output Window is the primary source of information when using EPICpy. Text printed to this window conveys the output of various menu operations, e.g., this is the result of loading the Choice Task device and ruleset: Any EPIClib or EPICapp errors, will also be printed to the Normal Output Window. E.g., the following results from a production rule formatting error: While running a simulation, the Normal Output Window often displays a running trace of EPIC's Working Memories for each simulated 50ms step. E.g. (slowed down): Trace Output Window The Trace Output Window is used to monitor the detailed operation of various EPIC processors. By default, the Trace Output Window will be dormant, but tracing of various processors can be enabled using the Settings\u27a1Trace Settings dialog. For example, the following configuration will enable tracing of EPIC's visual and auditory processors: As a result, the following may be viewed during a simulation run (slowed down): Stats Output Window The Stats Output Window content is entirely dependent on the device. Some devices may not use this window, others may use it to show text, tables, figures, and other depictions of a model's performance. Typically, the Stats Output Window will be updated only occasionally (for example, after a simulation run). More frequent updates will significantly slow down the simulation. The following shows a possible output following several runs of the Choice Task model: Window Positioning By default, the window layout of EPICapp is as follows and appears on the first monitor available: If this layout is altered by closing, moving, or resizing windows, when no device is loaded , the default window layout will be updated and remembered when EPICapp is next loaded. To return to the default layout, use the menu option Windows\u27a1Restore Default Layout and then close EPICpy without loading a device. If the layout is altered by closing, moving, or resizing windows while a device is loaded , then that layout will be reinstated the next time that device is loaded. Again, to restore the default layout for a particular device, just load that device, choose Windows\u27a1Restore Default Layout , then either close EPICpy or load a different device to store that device's window settings anew. E.g., the following is an example custom layout for the Choice Task device: This layout ignores the auditory view windows and focuses on the Visual Physical View and Trace windows.","title":"EPICpy GUI Windows"},{"location":"epicinterface_windows/#epicpy-interface","text":"When you first run EPICpy, the graphical user interface (GUI) should look like this:","title":"EPICpy Interface"},{"location":"epicinterface_windows/#model-view-windows","text":"During simulations, EPICpy allows the modeler to monitor content being represented by 3 key processors within the EPIC architecture; The Physical View, the Sensory View, and the Perceptual View. Visual Views During the Williams Task For example, the figure above depicts a snapshot from a simulation of the paradigm from Williams (1967) . On each trial, a large assortment of objects with various colors, shapes, and sizes are displayed in a grid configuration. Prior to this display, participants are asked to locate a particular object (e.g., medium, plus, green) and respond with a button press a soon as they have found it. The Physical View : The Physical View shows all visual information currently being represented on the display of the simulated device. The first panel of the figure above shows the Visual Physical View of the Williams Task. Notice that the current gaze position of EPIC is represented by a set of concentric circles. The center-most point of these circles represents the location of EPIC's current gaze point. The small filled grey circle extends to 1 degree of visual angle (DVA) around this gaze-point. The larger unfilled circle extends to 7.6 DVA. Modellers can use these circles to tell where EPIC is currently \"looking\". Like a computer monitor, when visual information is drawn on the device's virtual monitor, it is visible immediately on the Visual Physical View. Similarly, when information is removed from the simulated monitor, it is removed immediately from the Visual Physical View. The Sensory View : The second panel of the figure above shows the Visual Sensory View of the Williams Task. The Sensory View allows the Modeller to monitor which subset of the information displayed on the Visual Physical View is currently being represented by EPIC's ocular system. There is a delay between the time information appears on the device and when it can be represented by the eye, and so information on the Visual Sensory View will lag that on the Visual Physical View. This is also true for information removed from the device, there will be a short delay before it is no longer supported by the visual system. Note that in this snapshot of the Williams Task, EPIC only has complete information (shape, color, and text) of the shape directly in front of its fovea, partial information (e.g., shape) out to about 7 or so degrees, and beyond that it represents no information. Once the eye moves to a new location, and after appropriate psychological delays, it will only represent ocular information available at that location. Exactly what EPIC can see from a particular gaze location may be affected by the specific type and parameterization of any availability functions currently defined in the rules. The Perceptual View : The third panel of the figure above shows the Visual Perceptual View of the Williams Task. Just as EPIC's sensory system can only represent information available on the virtual display, EPIC's perceptual system can only represent information previously represented by the sensory system. The Visual Perceptual View does not represent the entire contents of Visual Perception (for that one would have to view the Working Memory trace; see below). However, visual object properties such as location, shape, size, relative position, color, text, etc. are depicted here. Although both EPIC's sensory system and its perceptual system maintain information even after it is no longer supported on the device, perceptual information stays around for considerably longer. Because of this, the Visual Perceptual View will show information about many of the objects that have been recently foveated by EPIC's visual system. Williams Task In Action (Slowed Down Greatly) The video above shows a set of trials of the Williams Task. EPIC first reads each of the 3 words that indicate which shape is the current trial's target stimulus. Notice the temporal dynamics of the 3 view windows; information presented on the display shows up on the Sensory Vision window after a delay, and on the Perception Vision window after another delay. After information is removed from the virtual device, it then disappears from the Sensory Vision window, and then later from the Perceptual Vision window. Auditory Views During the Choice Task There are also 3 corresponding view windows for EPIC's auditory system. The Auditory Physical View depicts any audio played by the virtual device. These can be sounds from a variety of virtual sources, including speakers, headphones, human speakers, nature, etc., and are represented as either a sound or speech within EPIC. No matter the source, sounds played by the device are depicted for the modeller as text located somewhere on the 2D space of the Auditory Physical View window. This text contains the name of the sound, the timbre (sounds) or text content (speech), as well as several other pieces of information (e.g., loudness, pitch, etc.). The exact set of information displayed for auditory objects is configurable via the Settings\u27a1Sound Text Settings dialog. If all available information is enabled, the Auditory View Windows may become too congested to follow. In the default Choice Task device, each trial's visual fixation is accompanied by a computerized speech sound of the word \"Warning\". This is denoted by Speaker: Computer, Content: Warning, Kind: Speech . Each trial's stimulus is accompanied by a non-speech sound with a timbre dependent on the stimulus color (in this case, \"Beep\"). This is denoted by Timbre: Beep, Stream: Signal, Kind: Sound . This particular output for both speech and sound objects is a subset of what is available in the Settings\u27a1Sound Text Settings dialog. For example, this particular output is achieved with the following setting: Note that Timbre and Content are grayed out because they cannot be disabled for sounds and speech, respectively.","title":"Model View Windows"},{"location":"epicinterface_windows/#normal-output-window","text":"Then Normal Output Window is the primary source of information when using EPICpy. Text printed to this window conveys the output of various menu operations, e.g., this is the result of loading the Choice Task device and ruleset: Any EPIClib or EPICapp errors, will also be printed to the Normal Output Window. E.g., the following results from a production rule formatting error: While running a simulation, the Normal Output Window often displays a running trace of EPIC's Working Memories for each simulated 50ms step. E.g. (slowed down):","title":"Normal Output Window"},{"location":"epicinterface_windows/#trace-output-window","text":"The Trace Output Window is used to monitor the detailed operation of various EPIC processors. By default, the Trace Output Window will be dormant, but tracing of various processors can be enabled using the Settings\u27a1Trace Settings dialog. For example, the following configuration will enable tracing of EPIC's visual and auditory processors: As a result, the following may be viewed during a simulation run (slowed down):","title":"Trace Output Window"},{"location":"epicinterface_windows/#stats-output-window","text":"The Stats Output Window content is entirely dependent on the device. Some devices may not use this window, others may use it to show text, tables, figures, and other depictions of a model's performance. Typically, the Stats Output Window will be updated only occasionally (for example, after a simulation run). More frequent updates will significantly slow down the simulation. The following shows a possible output following several runs of the Choice Task model:","title":"Stats Output Window"},{"location":"epicinterface_windows/#window-positioning","text":"By default, the window layout of EPICapp is as follows and appears on the first monitor available: If this layout is altered by closing, moving, or resizing windows, when no device is loaded , the default window layout will be updated and remembered when EPICapp is next loaded. To return to the default layout, use the menu option Windows\u27a1Restore Default Layout and then close EPICpy without loading a device. If the layout is altered by closing, moving, or resizing windows while a device is loaded , then that layout will be reinstated the next time that device is loaded. Again, to restore the default layout for a particular device, just load that device, choose Windows\u27a1Restore Default Layout , then either close EPICpy or load a different device to store that device's window settings anew. E.g., the following is an example custom layout for the Choice Task device: This layout ignores the auditory view windows and focuses on the Visual Physical View and Trace windows.","title":"Window Positioning"},{"location":"epicpy_development_setup/","text":"EPICpy Development Overview EPICpy is developed using the following technologies: Require Python Versions Version 3.9.16 on Windows Version 3.10.11 on Linux and MacOS Why these versions? The upper limit is 3.10 because some modules we use are not currently rewritten for Python 3.11 or are not sufficiently stable. When this changes, the version will be switched to 3.11 on MacOS and Linux. On Windows, the supported Python version is 3.9 because AFAIK, Microsoft Visual Studio 2019 built in Python (required at the moment to build EPICLib with PyBind11) is limited to 3.9. Our goal is to switch to the latest version of MSVS shortly which may support Python 3.10 and allow EPICpy to run with Python 3.10 on Windows. Key Packages PyBind11 (pybind11): Library for generating Python bindings for existing C++ code. We are currently using this to produce a Python3.9 compatible library of EPIClib for Windows and a Python3.10 compatible library for MacOS and Linux. PySide2 GUI Framework (pyside2): Used to build the Graphical User Interface. Note: We are momentarily using PyQt5 instead of PySide2, but this will change shortly and allow for an overall more permissive license for EPICpy . Pingouin Stats System (pingouin): Meta-package providing access variety of statistical and graphing packages. PipX (pipx): Allows easy cross-platform distributing Python applications. Setup Dev Environment Linux Tested on these Linux Variants [x] Debian-based Linux variants (e.g., Ubuntu, Mint, PopOS, etc.) [ ] Redhat-based Linux variants (e.g., Redhat, Fedora, Centos, etc.) [ ] Arch-based Linux variants (e.g., ArchLinux, Manjaro, EndeavourOS, etc.) Tested on these Linux Versions: [x] 18.04 [x] 20.04 [x] 22.04 Most recent Linux distributions come with Python 3 already installed, so you probably already have Python. However, you will need to make sure you build your virtual environment using Python 3.10. In particular, adjust the path of the python3 -m venv venv command below so that it points to the correct version of Python (or use your own method to create the proper virtual environment).. Install Prerequisites If you haven't already installed them, then you already have these), you should install the following: sudo apt install build-essential libssl-dev libffi-dev libncurses5-dev zlib1g zlib1g-dev libreadline-dev libbz2-dev libsqlite3-dev make gcc curl git You will also need to install the PyQt5 Development files: sudo apt install qtbase5-dev Retrieve Source Files from GitHub # note: this will create the EPICpy source folder in the directory from which you type these commands: git clone https://github.com/travisseymour/EPICpy.git cd EPICpy Set Up A Virtual Environment NOTE: this section assumes you already have Python 3.10 on your machine and know the path or command required to start it. Set up a Virtual Environment Using Python 3.10 Create the virtual environment cd EPICpy # vvv replace with path to your copy of Python 3.10 ~/.pyenv/versions/3.10.11/bin/python3 -m venv venv Activate the virtual environment source venv/bin/activate Check it python -V This should report Python 3.10.x Install Dependencies Activate the virtual environment source venv/bin/activate Update Python package manager # update pip and wheel pip install -U pip wheel Install the EPICpy Python requirements # install EPICpy requirements pip install -r requirements.txt Setup EPICpy in Your Python IDE The following assumes you are following along with the PyCharm Integrated Python Development Environment . For others (e.g., VSCode, Sublime, etc.) should modify the instructions accordingly. Obtain PyCharm's Free Community Edition IDE # If you like using Canonical's proprietary Snap Linux app installer # To install snap https://itsfoss.com/install-snap-linux/ sudo snap install pycharm-community --classic # If you like to live the FOSS way using the Flatpak Linux app installer # To install flatpak https://itsfoss.com/flatpak-guide/ flatpak install com.jetbrains.PyCharm-Community Start PyCharm Open The EPICpy folder as a new PyCharm Project Open The File Viewer Set Up The Interpreter Click on the \\<NoInterpreter> button in the bottom right of the IDE, choose AddNewInterpreter , then AddLocalInterpreter : Choose Existing , make sure the Location field contains the path to the Python 3.10 virtual environment ( venv folder) you created earlier and press OK : Set The Sources Root Set the Sources Root by right-clicking on the epicpy folder and choosing MarkDirectoryAs-->SourcesRoot . Run EPICpy Finally, right-click the file main.py and choose Run main If all goes well, you should be looking at the EPICpy GUI. MacOS Tested on these Apple Chipsets: [x] Intel [ ] M1 [ ] M2 Tested on these MacOS Versions: [x] 10.15 Catalina [ ] 11 Big Sur [ ] 12 Monterey Most recent MacOS distributions come with Python 3 already installed, so you probably already have Python. However, you will need to make sure you build your virtual environment below using Python 3.10. In particular, adjust the path of the python3 -m venv venv command below so that it points to the correct version of Python (or use your own method to create the proper virtual environment). Install Prerequisites You will need to install the XCode commandline build utilities. A super easy way to do this is to just install the Homebrew package manager. You may have already done this, if not see this website (https://brew.sh/). Another approach is to install XCode (it's a 10GB download, and you'll need an additional 10GB of space to install it) from the Apple Store and then use XCode to install the commandline utilities. This is challenging if you don't have the latest version of MacOS, as the version of XCode in the Apple Store may not be installable. Generally, I strongly recommend installing Homebrew(https://brew.sh/); not only does it install the build tools you need, but it's much faster than installing XCode, will install on any version of MacOS. Besides, having Homebrew is super useful anyway. If you already have XCode installed, you may be good to go. Retrieve Source Files from GitHub # note: this will create the EPICpy source folder in the directory from which you type these commands: git clone https://github.com/travisseymour/EPICpy.git cd EPICpy Set Up A Virtual Environment NOTE: this section assumes you already have Python 3.10 on your machine and know the path or command required to start it. Set up a Virtual Environment Using Python 3.10 Create the virtual environment cd EPICpy # vvv replace with path to your copy of Python 3.10 ~/.pyenv/versions/3.10.11/bin/python3 -m venv Activate the virtual environment source venv/bin/activate Check it python -V This should report Python 3.10.x Install Dependencies Activate the virtual environment source venv/bin/activate Update Python package manager # update pip and wheel pip install -U pip wheel Install the EPICpy Python requirements # install EPICpy requirements pip install -r requirements.txt # If you have an fbs pro install command, run that now. It can also be upgraded later if you purchased fbs pro (optional). Setup EPICpy in Your Python IDE The following assumes you are following along with the PyCharm Integrated Python Development Environment . For others (e.g., VSCode, Sublime, etc.) should modify the instructions accordingly. Obtain PyCharm's Free Community Edition IDE brew install --cask pycharm-ce Start PyCharm Open The EPICpy folder as a new PyCharm Project Open The File Viewer Set Up The Python Interpreter Click on the \\<NoInterpreter> button in the bottom right of the IDE, choose AddNewInterpreter , then AddLocalInterpreter : Choose Existing , make sure the Location field contains the path to the Python 3.10 virtual environment ( venv folder) you created earlier and press OK : Set Sources Root Set the Sources Root by right-clicking on the epicpy folder and choosing MarkDirectoryAs-->SourcesRoot . Run EPICpy Finally, right-click the file main.py and choose Run main If all goes well, you should be looking at the EPICpy GUI. Windows Tested on these Windows Versions: [x] 10 [ ] 11 Although Windows doesn't ship with Python 3 already installed, you may have previously installed Python. However, you will need to make sure you build your virtual environment below using Python 3.9. In particular, adjust the path of the python3 -m venv venv command below so that it points to the correct version of Python (or use your own method to create the proper virtual environment). Install Prerequisites Windows doesn't come with git installed. You will need git to obtain the EPICpy files from the software repository on GitHub.com. You can install git by using an installer from this website (https://git-scm.com/downloads) Retrieve Files from GitHub # note: this will create the EPICpy source folder in the directory from which you type these commands: git clone https://github.com/travisseymour/EPICpy.git cd EPICpy Set Up A Virtual Environment NOTE: this section assumes you already have Python 3.10 on your machine and know the path or command required to start it. If not, install uv and then un uv python install 3.10 Set up a Virtual Environment Using Python 3.10 Create the virtual environment cd EPICpy # vvv replace with path to your copy of Python 3.10 C:\\Users\\testuser\\.pyenv\\pyenv-win\\versions\\3.9.13\\python.exe -m venv venv Activate the virtual environment venv/Scripts/activate Check it python -V This should report Python 3.9.x Install Dependencies Activate the virtual environment source venv/bin/activate Update Python package manager # update pip and wheel pip install -U pip wheel Install the EPICpy Python requirements # install EPICpy requirements pip install -r requirements.txt Setup EPICpy in Your Python IDE The following assumes you are following along with the PyCharm Integrated Python Development Environment . For others (e.g., VSCode, Sublime, etc.) should modify the instructions accordingly. Obtain PyCharm's Free Community Edition IDE Go to the JetBrains website, scroll down and see the instructions for PyCharm Community Edition: https://www.jetbrains.com/pycharm/download/ Start PyCharm Open The EPICpy folder as a new PyCharm Project Open The File Viewer Setup UP Python Interpreter Click on the \\<NoInterpreter> button in the bottom right of the IDE, choose AddNewInterpreter , then AddLocalInterpreter : If instead of \\<NoInterpreter> it says \"Python3.9(EPICpy)\" then PyCharm has already found and correctly installed your virtual environment! Otherwise, continue setting up the interpreter: Choose Existing , make sure the Location field contains the path to the Python 3.10 virtual environment ( venv folder) you created earlier and press OK : Set Sources Root Set the Sources Root by right-clicking on the epicpy folder and choosing MarkDirectoryAs-->SourcesRoot . Run EPICpy Finally, right-click the file main.py and choose Run main If all goes well, you should be looking at the EPICpy GUI.","title":"EPICpy Development Setup"},{"location":"epicpy_development_setup/#epicpy-development","text":"","title":"EPICpy Development"},{"location":"epicpy_development_setup/#overview","text":"EPICpy is developed using the following technologies: Require Python Versions Version 3.9.16 on Windows Version 3.10.11 on Linux and MacOS Why these versions? The upper limit is 3.10 because some modules we use are not currently rewritten for Python 3.11 or are not sufficiently stable. When this changes, the version will be switched to 3.11 on MacOS and Linux. On Windows, the supported Python version is 3.9 because AFAIK, Microsoft Visual Studio 2019 built in Python (required at the moment to build EPICLib with PyBind11) is limited to 3.9. Our goal is to switch to the latest version of MSVS shortly which may support Python 3.10 and allow EPICpy to run with Python 3.10 on Windows. Key Packages PyBind11 (pybind11): Library for generating Python bindings for existing C++ code. We are currently using this to produce a Python3.9 compatible library of EPIClib for Windows and a Python3.10 compatible library for MacOS and Linux. PySide2 GUI Framework (pyside2): Used to build the Graphical User Interface. Note: We are momentarily using PyQt5 instead of PySide2, but this will change shortly and allow for an overall more permissive license for EPICpy . Pingouin Stats System (pingouin): Meta-package providing access variety of statistical and graphing packages. PipX (pipx): Allows easy cross-platform distributing Python applications.","title":"Overview"},{"location":"epicpy_development_setup/#setup-dev-environment","text":"","title":"Setup Dev Environment"},{"location":"epicpy_development_setup/#linux","text":"","title":"Linux"},{"location":"epicpy_development_setup/#tested-on-these-linux-variants","text":"[x] Debian-based Linux variants (e.g., Ubuntu, Mint, PopOS, etc.) [ ] Redhat-based Linux variants (e.g., Redhat, Fedora, Centos, etc.) [ ] Arch-based Linux variants (e.g., ArchLinux, Manjaro, EndeavourOS, etc.) Tested on these Linux Versions: [x] 18.04 [x] 20.04 [x] 22.04 Most recent Linux distributions come with Python 3 already installed, so you probably already have Python. However, you will need to make sure you build your virtual environment using Python 3.10. In particular, adjust the path of the python3 -m venv venv command below so that it points to the correct version of Python (or use your own method to create the proper virtual environment)..","title":"Tested on these Linux Variants"},{"location":"epicpy_development_setup/#install-prerequisites","text":"If you haven't already installed them, then you already have these), you should install the following: sudo apt install build-essential libssl-dev libffi-dev libncurses5-dev zlib1g zlib1g-dev libreadline-dev libbz2-dev libsqlite3-dev make gcc curl git You will also need to install the PyQt5 Development files: sudo apt install qtbase5-dev","title":"Install Prerequisites"},{"location":"epicpy_development_setup/#retrieve-source-files-from-github","text":"# note: this will create the EPICpy source folder in the directory from which you type these commands: git clone https://github.com/travisseymour/EPICpy.git cd EPICpy","title":"Retrieve Source Files from GitHub"},{"location":"epicpy_development_setup/#set-up-a-virtual-environment","text":"NOTE: this section assumes you already have Python 3.10 on your machine and know the path or command required to start it.","title":"Set Up A Virtual Environment"},{"location":"epicpy_development_setup/#set-up-a-virtual-environment-using-python-310","text":"Create the virtual environment cd EPICpy # vvv replace with path to your copy of Python 3.10 ~/.pyenv/versions/3.10.11/bin/python3 -m venv venv Activate the virtual environment source venv/bin/activate Check it python -V This should report Python 3.10.x","title":"Set up a Virtual Environment Using Python 3.10"},{"location":"epicpy_development_setup/#install-dependencies","text":"Activate the virtual environment source venv/bin/activate Update Python package manager # update pip and wheel pip install -U pip wheel Install the EPICpy Python requirements # install EPICpy requirements pip install -r requirements.txt","title":"Install Dependencies"},{"location":"epicpy_development_setup/#setup-epicpy-in-your-python-ide","text":"The following assumes you are following along with the PyCharm Integrated Python Development Environment . For others (e.g., VSCode, Sublime, etc.) should modify the instructions accordingly.","title":"Setup EPICpy in Your Python IDE"},{"location":"epicpy_development_setup/#obtain-pycharms-free-community-edition-ide","text":"# If you like using Canonical's proprietary Snap Linux app installer # To install snap https://itsfoss.com/install-snap-linux/ sudo snap install pycharm-community --classic # If you like to live the FOSS way using the Flatpak Linux app installer # To install flatpak https://itsfoss.com/flatpak-guide/ flatpak install com.jetbrains.PyCharm-Community","title":"Obtain PyCharm's Free Community Edition IDE"},{"location":"epicpy_development_setup/#start-pycharm","text":"","title":"Start PyCharm"},{"location":"epicpy_development_setup/#open-the-epicpy-folder-as-a-new-pycharm-project","text":"","title":"Open The EPICpy folder as a new PyCharm Project"},{"location":"epicpy_development_setup/#open-the-file-viewer","text":"","title":"Open The File Viewer"},{"location":"epicpy_development_setup/#set-up-the-interpreter","text":"Click on the \\<NoInterpreter> button in the bottom right of the IDE, choose AddNewInterpreter , then AddLocalInterpreter : Choose Existing , make sure the Location field contains the path to the Python 3.10 virtual environment ( venv folder) you created earlier and press OK :","title":"Set Up The Interpreter"},{"location":"epicpy_development_setup/#set-the-sources-root","text":"Set the Sources Root by right-clicking on the epicpy folder and choosing MarkDirectoryAs-->SourcesRoot .","title":"Set The Sources Root"},{"location":"epicpy_development_setup/#run-epicpy","text":"Finally, right-click the file main.py and choose Run main If all goes well, you should be looking at the EPICpy GUI.","title":"Run EPICpy"},{"location":"epicpy_development_setup/#macos","text":"","title":"MacOS"},{"location":"epicpy_development_setup/#tested-on-these-apple-chipsets","text":"[x] Intel [ ] M1 [ ] M2 Tested on these MacOS Versions: [x] 10.15 Catalina [ ] 11 Big Sur [ ] 12 Monterey Most recent MacOS distributions come with Python 3 already installed, so you probably already have Python. However, you will need to make sure you build your virtual environment below using Python 3.10. In particular, adjust the path of the python3 -m venv venv command below so that it points to the correct version of Python (or use your own method to create the proper virtual environment).","title":"Tested on these Apple Chipsets:"},{"location":"epicpy_development_setup/#install-prerequisites_1","text":"You will need to install the XCode commandline build utilities. A super easy way to do this is to just install the Homebrew package manager. You may have already done this, if not see this website (https://brew.sh/). Another approach is to install XCode (it's a 10GB download, and you'll need an additional 10GB of space to install it) from the Apple Store and then use XCode to install the commandline utilities. This is challenging if you don't have the latest version of MacOS, as the version of XCode in the Apple Store may not be installable. Generally, I strongly recommend installing Homebrew(https://brew.sh/); not only does it install the build tools you need, but it's much faster than installing XCode, will install on any version of MacOS. Besides, having Homebrew is super useful anyway. If you already have XCode installed, you may be good to go.","title":"Install Prerequisites"},{"location":"epicpy_development_setup/#retrieve-source-files-from-github_1","text":"# note: this will create the EPICpy source folder in the directory from which you type these commands: git clone https://github.com/travisseymour/EPICpy.git cd EPICpy","title":"Retrieve Source Files from GitHub"},{"location":"epicpy_development_setup/#set-up-a-virtual-environment_1","text":"NOTE: this section assumes you already have Python 3.10 on your machine and know the path or command required to start it.","title":"Set Up A Virtual Environment"},{"location":"epicpy_development_setup/#set-up-a-virtual-environment-using-python-310_1","text":"Create the virtual environment cd EPICpy # vvv replace with path to your copy of Python 3.10 ~/.pyenv/versions/3.10.11/bin/python3 -m venv Activate the virtual environment source venv/bin/activate Check it python -V This should report Python 3.10.x","title":"Set up a Virtual Environment Using Python 3.10"},{"location":"epicpy_development_setup/#install-dependencies_1","text":"Activate the virtual environment source venv/bin/activate Update Python package manager # update pip and wheel pip install -U pip wheel Install the EPICpy Python requirements # install EPICpy requirements pip install -r requirements.txt # If you have an fbs pro install command, run that now. It can also be upgraded later if you purchased fbs pro (optional).","title":"Install Dependencies"},{"location":"epicpy_development_setup/#setup-epicpy-in-your-python-ide_1","text":"The following assumes you are following along with the PyCharm Integrated Python Development Environment . For others (e.g., VSCode, Sublime, etc.) should modify the instructions accordingly.","title":"Setup EPICpy in Your Python IDE"},{"location":"epicpy_development_setup/#obtain-pycharms-free-community-edition-ide_1","text":"brew install --cask pycharm-ce","title":"Obtain PyCharm's Free Community Edition IDE"},{"location":"epicpy_development_setup/#start-pycharm_1","text":"","title":"Start PyCharm"},{"location":"epicpy_development_setup/#open-the-epicpy-folder-as-a-new-pycharm-project_1","text":"","title":"Open The EPICpy folder as a new PyCharm Project"},{"location":"epicpy_development_setup/#open-the-file-viewer_1","text":"","title":"Open The File Viewer"},{"location":"epicpy_development_setup/#set-up-the-python-interpreter","text":"Click on the \\<NoInterpreter> button in the bottom right of the IDE, choose AddNewInterpreter , then AddLocalInterpreter : Choose Existing , make sure the Location field contains the path to the Python 3.10 virtual environment ( venv folder) you created earlier and press OK :","title":"Set Up The Python Interpreter"},{"location":"epicpy_development_setup/#set-sources-root","text":"Set the Sources Root by right-clicking on the epicpy folder and choosing MarkDirectoryAs-->SourcesRoot .","title":"Set Sources Root"},{"location":"epicpy_development_setup/#run-epicpy_1","text":"Finally, right-click the file main.py and choose Run main If all goes well, you should be looking at the EPICpy GUI.","title":"Run EPICpy"},{"location":"epicpy_development_setup/#windows","text":"","title":"Windows"},{"location":"epicpy_development_setup/#tested-on-these-windows-versions","text":"[x] 10 [ ] 11 Although Windows doesn't ship with Python 3 already installed, you may have previously installed Python. However, you will need to make sure you build your virtual environment below using Python 3.9. In particular, adjust the path of the python3 -m venv venv command below so that it points to the correct version of Python (or use your own method to create the proper virtual environment).","title":"Tested on these Windows Versions:"},{"location":"epicpy_development_setup/#install-prerequisites_2","text":"Windows doesn't come with git installed. You will need git to obtain the EPICpy files from the software repository on GitHub.com. You can install git by using an installer from this website (https://git-scm.com/downloads)","title":"Install Prerequisites"},{"location":"epicpy_development_setup/#retrieve-files-from-github","text":"# note: this will create the EPICpy source folder in the directory from which you type these commands: git clone https://github.com/travisseymour/EPICpy.git cd EPICpy","title":"Retrieve Files from GitHub"},{"location":"epicpy_development_setup/#set-up-a-virtual-environment_2","text":"NOTE: this section assumes you already have Python 3.10 on your machine and know the path or command required to start it. If not, install uv and then un uv python install 3.10","title":"Set Up A Virtual Environment"},{"location":"epicpy_development_setup/#set-up-a-virtual-environment-using-python-310_2","text":"Create the virtual environment cd EPICpy # vvv replace with path to your copy of Python 3.10 C:\\Users\\testuser\\.pyenv\\pyenv-win\\versions\\3.9.13\\python.exe -m venv venv Activate the virtual environment venv/Scripts/activate Check it python -V This should report Python 3.9.x","title":"Set up a Virtual Environment Using Python 3.10"},{"location":"epicpy_development_setup/#install-dependencies_2","text":"Activate the virtual environment source venv/bin/activate Update Python package manager # update pip and wheel pip install -U pip wheel Install the EPICpy Python requirements # install EPICpy requirements pip install -r requirements.txt","title":"Install Dependencies"},{"location":"epicpy_development_setup/#setup-epicpy-in-your-python-ide_2","text":"The following assumes you are following along with the PyCharm Integrated Python Development Environment . For others (e.g., VSCode, Sublime, etc.) should modify the instructions accordingly.","title":"Setup EPICpy in Your Python IDE"},{"location":"epicpy_development_setup/#obtain-pycharms-free-community-edition-ide_2","text":"Go to the JetBrains website, scroll down and see the instructions for PyCharm Community Edition: https://www.jetbrains.com/pycharm/download/","title":"Obtain PyCharm's Free Community Edition IDE"},{"location":"epicpy_development_setup/#start-pycharm_2","text":"","title":"Start PyCharm"},{"location":"epicpy_development_setup/#open-the-epicpy-folder-as-a-new-pycharm-project_2","text":"","title":"Open The EPICpy folder as a new PyCharm Project"},{"location":"epicpy_development_setup/#open-the-file-viewer_2","text":"","title":"Open The File Viewer"},{"location":"epicpy_development_setup/#setup-up-python-interpreter","text":"Click on the \\<NoInterpreter> button in the bottom right of the IDE, choose AddNewInterpreter , then AddLocalInterpreter : If instead of \\<NoInterpreter> it says \"Python3.9(EPICpy)\" then PyCharm has already found and correctly installed your virtual environment! Otherwise, continue setting up the interpreter: Choose Existing , make sure the Location field contains the path to the Python 3.10 virtual environment ( venv folder) you created earlier and press OK :","title":"Setup UP Python Interpreter"},{"location":"epicpy_development_setup/#set-sources-root_1","text":"Set the Sources Root by right-clicking on the epicpy folder and choosing MarkDirectoryAs-->SourcesRoot .","title":"Set Sources Root"},{"location":"epicpy_development_setup/#run-epicpy_2","text":"Finally, right-click the file main.py and choose Run main If all goes well, you should be looking at the EPICpy GUI.","title":"Run EPICpy"},{"location":"epicpysimcomponents/","text":"EPICpy Simulation Components This page describes of what it means to simulate human performance using EPIC under EPICpy, by going over each of the key components of a simulation run. For an overview of what EPIC and EPICpy are, and what they are used for, see the Home Page . To see an example simulation walkthrough, see the Epic Simulation Example page. Figure 1 Executive-Process/Interactive-Control (EPIC) is a Cognitive Architecture created by David Kieras and David Meyer. EPIC describes both an integrated theory of human performance and a computational approach to using that system to simulate human behavior. Detailed description of the EPIC architecture as a theory of human performance, and examples of how it can be applied to answering various psychological questions can be found at https://web.eecs.umich.edu/~kieras/epic.html (also see the EPIC Resources page). EPIC itself is a library written in C++ (called EPICLib), but cannot be used directly. Instead, you need some control software that allows you to configure and manage simulations using EPIC. Previously, EPIC the architecture was combined with EPICapp, the GUI-based control software, also written in C++. This combination can be found at https://github.com/dekieras/EPIC (requires MacOS with XCode installed). Using EPICpy, Like EPICapp before it, generally involves the components shown in Figure 1, expect that some elements are now written in Python rather than C++: EPICpy Application The EPICpy Application contains both an interface to EPIC architecture (copied directly from the C++ code maintained by its creator), which one only accesses through Task-Device programming. The EPICpy Application also offers a graphical user interface (GUI) for managing a simulation run. When launched, the EPICpy GUI will look similar to Figure 2: Figure 2 Notice that there are 3 windows for Vision, and 3 windows for Audition. For each modality, there is the Physical window (a depiction of what's on the task-device display screen), a Sensory Window (a depiction of what information is currently being represented within EPIC's sensory processors), and a Perceptual Window (a depiction of what information is currently represented within EPIC's perceptual processors). There is also the Normal Output window that shows most information about the simulation status, errors, etc. It also shows a continual trace of the current contents of EPIC's Working Memory (updated every 50 ms) while a simulation is running. The Trace Output window shows detailed trace information for various EPIC processors, but only if they are enabled in the settings. By default, no trace information will be displayed. The Stats Output window shows any statistical analysis information written by the device and can include text, tables, and graphs. EPICpy's GUI allows setting various general options via the Settings menu, as well as various model-specific options via the Settings and Run menus. The GUI also facilitates loading Task Devices, Task Rules, and Perceptual Encoders, which together represent a single psychological model for simulation. Task Devices The File\u27a1Load_Device menu item is used to load a Task Device into the simulation environment. A task device is a Python code file that represents a simulated performance environment that EPIC (the simulated human performer) will interact with. The device represents the hardware and software that users interact with, and can present visual and auditory stimuli, manual and vocal response devices, and task logic. Thus, the task device controls the progression of a task and dictates when various stimuli are shown and how it handles inputs from EPIC. For example, researchers may run human participants in trials of a task using a computer monitor and speakers to display stimuli and warnings, as well as a keyboard to register responses. Data from this human run would be collected and stored. To understand performance in such a task, researcher might produce a hypothesis based on theory or performance in similar tasks. They may wish to test their hypothesis for how humans performed this task by simulating that performance. The task device would be constructed to mimic the human task device (again, including all relevant inputs and outputs, as well as corresponding data collection). The task device would be programmed to sufficient detail to serve as a reasonable test of the hypothesized performance strategy, but may omit details deemed insignificant. E.g., your hypothesized model for how one uses a word processor may omit representation of the web camera affixed to the top of the monitor if the presence of that camera is presumed to have no significant effect on task performance. Similarly, one might omit texture and shading in a model of driving performance, if these visual details are deemed irrelevant. Of course assumption about what to include, or exclude from the simulated task device could be incorrect, which is why such assumptions are effectively a part of the psychological model being tested. Currently, EPIC (the architecture) allows flexible object-based representations of visual and auditory stimuli (arbitrary sounds and speech) localized in space relative to the simulated performer. EPIC also allows a number of manual response devices (e.g., response-box, keyboard, joystick, mouse movement, etc.), each with multiple response styles. EPIC also supports ocular movements and vocal responses. Thus, these inputs and outputs are the ones available to task devices. For information on how to program EPICpy devices, see Programming Overview section. Perceptual Encoders Although EPICpy does not provide a way for modelers to alter EPIC itself (the architecture which is programmed in C++ and imported into EPICpy as a compiled, shared library), there is a way to write Python code that alters perceptual processing within EPIC. A Perceptual Encoder is a set of Python code that represents a psychological model for how some stimulus features are encoded into EPIC's Working Memory following sensation and early perceptual processes, but prior to Worming Memory representation. For example, if a furniture novice is shown a display containing a number of furniture items, these objects may be encoded into working memory as [\"chair\", \"table\", \"sofa\", \"lamp\"]. A furniture expert may have enough experience that they instead perceive the same visual items as [\"Eames\", \"VanDerRohe\", \"KnollBassett\", \"Arco\"]. This transformation could be embodied in a Visual Encoder. Other visual encoders may make adjustments for participants with red-green color blindness, or add error to object spatial localization due to lack of experience or fatigue. Thus, an encoder translates or alters perceptual information before it is added to EPIC's Working Memory. EPIC also allows Auditory Encoders that do the same kind of transform but for auditory information. For example, an auditory encoder could be used to represent how people mis-hear certain words, or mis-localize certain sounds. Similarly, an auditory encoder can be used to perceive tones of 440, 880, and 1600 Hertz and perceive them as 'high', 'medium', and 'low'. EPICpy allows the simultaneous use of Visual and Auditory encoders. Encoders can be loaded using the File\u27a1Load_Visual_Encoder , and File\u27a1Load_Auditory_Encoder menu items. These items are only accessible after a device has been loaded. On subsequent device loads, EPICpy will automatically re-establish previously used encoders. Encoders can also be unloaded from a simulation using the File\u27a1Unload_Visual_Encoder , and File\u27a1Unload_Auditory_Encoder menu items. For information on how to program EPICpy encoders, see Programming Overview section. Task Rules Whereas the Task Device simulates the task performance context (inputs, outputs, sequence, data collection and scoring, stopping rules, etc.) the Task Rules represent a strategy for performing the task. These rules can either represent conscious strategies one uses to follow task rules and meet task goals, or non-conscious cognitive processing that occurs during task performance. Task Rules are written generally in a format consistent with other \"Production Rules\" used in other simulation systems. However, the specific syntax and grammar EPIC expects is defined as part of the Parsimonious Production System (PPS) by David Kieras (2004). To learn how to read/write production rules in EPIC, one can read the EPIC Principles of Operation (2004) by David Kieras. Although unnecessary for most modelers, to dive deeper and understand how PPS works you can read PPS: A Parsimonious Production System (1987) by Arie Covrigaru and David Kieras. Task rules are loaded using the File\u27a1Compile_Rules menu item. Rule files are typically plain text files with a .prs file extension. Rule files may contain any number of production rules, each with a simple IF/THEN organization. The IF section contains a set of clauses. If each of these clauses is true for the contents of EPIC's WM when evaluated, the rule will \"fire\", that is, cary out each of the commands in the THEN section. For example: (VIS_visual_Fixation_onset If ( (Goal Do detection_task) (Step Waitfor Visual_Fixation_Onset) (Visual ?a Detection Onset) (Motor Ocular Processor Free) ) Then ( (Send_to_motor Ocular Perform Move ?a) (Add (Tag ?a Fixation_Symbol)) (Delete (Step Waitfor Visual_Fixation_Onset)) (Add (Step Waitfor Visual_Stimulus_Onset)) )) Rule files also contain other information that are not rules, but configuration information for EPIC. E.g.: Initial WM Contents The modeler may optionally specify some contents to be added to WM prior to simulation start using the Define Initial_memory_contents command. E.g., at the top of the rule file, on might include this line: (Define Initial_memory_contents (Goal Do detection_task)) EPIC Simulation Configuration In order to configure various aspects of EPIC's motor, sensory, and perceptual system, directives may be included at the top of a production rule file in a Define Parameters clause. E.g.: (Define Parameters # Specify distribution governing visual perceptual recoding rate (Visual_perceptual_processor Recoding_failure_rate1 Fixed 0.5) # Specify distribution governing manual motor accuracy (Manual Execution_fluctuation_factor Normal When_used 1.0 0.1) # Specify distribution visual sensory delays (Eye Property_delay_fluctuation_factor Normal When_used 1.0 0.1) ) Why would you want to alter the way EPIC's processors work when creating a simulation to model performance in a particular task? EPIC (the architecture) is programmed with various default parameters. E.g., perception of text by default occurs without error roughly 100 ms after sensation with very little temporal variation. Imagine this default was based on previous work using 12 point Arial font in black ink on a white display from 40 cm away. This default could be inappropriate for the task device you are using. E.g., a task using 8 point text written in medium gray ink on a light gray background, may result in slower perceptual encoding times, and greater encoding error and may require precise foveation to see. You can reflect this visual reality in the Define Parameters section. On the other extreme, imagine a task with 72 point font written in high contrast. It may be faster and less error-prone to encode such a stimulus. This section can also be used to specify the distribution parameters for availability functions. Availability functions dictate the probability that a perceptual property (e.g., color) will be represented by EPIC's ocular processor as a function of size and foveal eccentricity. For detail on EPIC parameters and availability functions, including how to specify them, see Kieras's EPIC Principles of Operation (2004) . Named Locations A third common directive modelers place at the top of their rules file sets up Named Locations. These named locations can be referred to in rules in order to, for example, direct eye movements or mouse responses. If you give participants instructions that refer to \"the center of the screen\", \"the menu area\", etc., their strategy to perform the task may be in reference to these named locations. To specify named locations, the Define Named_location directive is used, e.g.: (Define Named_location Center_screen 0.0 0.0) Putting It All Together So, an EPIC simulation involves a Task Device that represents a virtual version of a task. On each trial of the task, EPIC's behavior is dictated by the contents of the Task Rules. Operation of EPIC itself can be influenced by some combination of perceptual encoders and parameter directives. EPIC operates on 50 ms cycles; every 50 ms the contents of WM are compared to each rule in the rule file, any and all rules that match the current state of WM will fire and update WM and/or initiate some motor movement. This continues until the task is done, or nothing happens for a predetermined number of cycles (1000 by default). At the end of your simulation, the device would store any relevant data produced by EPIC (e.g., virtual response times, errors, mose movements, keyboard keys pressed, etc.). This virtual data can then be compared to the previously collected human data. If these data match closely, the strategy represented by the rule file is supported. If the data do not match, the hypothesized strategy may be incorrect or incomplete. The approach described above is a post-hoc way of using EPIC (human data first, generate hypothesized strategy after analyzing data, then verify understanding using an EPIC simulation). EPIC can also be used to do predictive modeling whereby simulated data is produced first, and then the generalizability of the corresponding production rules can be assessed by comparing data from later human participants. This might be more useful in a design context. E.g., EPIC may be used to iterate through various cockpit interface changes until a range of reasonable virtual performance is observed. Then a physical mockup of the task device can be created for testing humans and further refinement. Note that whereas task devices always specify stimuli to present to EPIC and response handlers to deal with responses EPIC makes, nothing else is guaranteed. This means that the kind of feedback, data storage, or post-task data analysis that is available is entirely up to the device programmer. Typically, devices represent a number of task trials (sometimes within a block structure), and records and stores trial-by-trial data to disk. In EPICpy, this will be stored in a file called data_output.csv within the same folder in which the device file is located. However, device programmers can decide whether to make use of this file, and may sometimes store other data files. Some devices present basic (often quite minimal) statistical analysis at the end of each run for quick verification of model performance. This analysis may simply indicate mean RT and number of correct responses. Sometimes more involved analyses are done using some basic facilities defined within the EPIC architecture (e.g., RMSE and other simple goodness-of-fit statistics). EPICpy has greatly expanded the type of analyses that can be done without reinventing the wheel by allowing model programmers to use a rich set of statistical and graphing packages available for statistics and graphing in Python. For more detail of the statistical facility in EPICpy, see the section on EPICpy's Stat Window .","title":"Simulation Components"},{"location":"epicpysimcomponents/#epicpy-simulation-components","text":"This page describes of what it means to simulate human performance using EPIC under EPICpy, by going over each of the key components of a simulation run. For an overview of what EPIC and EPICpy are, and what they are used for, see the Home Page . To see an example simulation walkthrough, see the Epic Simulation Example page. Figure 1 Executive-Process/Interactive-Control (EPIC) is a Cognitive Architecture created by David Kieras and David Meyer. EPIC describes both an integrated theory of human performance and a computational approach to using that system to simulate human behavior. Detailed description of the EPIC architecture as a theory of human performance, and examples of how it can be applied to answering various psychological questions can be found at https://web.eecs.umich.edu/~kieras/epic.html (also see the EPIC Resources page). EPIC itself is a library written in C++ (called EPICLib), but cannot be used directly. Instead, you need some control software that allows you to configure and manage simulations using EPIC. Previously, EPIC the architecture was combined with EPICapp, the GUI-based control software, also written in C++. This combination can be found at https://github.com/dekieras/EPIC (requires MacOS with XCode installed). Using EPICpy, Like EPICapp before it, generally involves the components shown in Figure 1, expect that some elements are now written in Python rather than C++:","title":"EPICpy Simulation Components"},{"location":"epicpysimcomponents/#epicpy-application","text":"The EPICpy Application contains both an interface to EPIC architecture (copied directly from the C++ code maintained by its creator), which one only accesses through Task-Device programming. The EPICpy Application also offers a graphical user interface (GUI) for managing a simulation run. When launched, the EPICpy GUI will look similar to Figure 2: Figure 2 Notice that there are 3 windows for Vision, and 3 windows for Audition. For each modality, there is the Physical window (a depiction of what's on the task-device display screen), a Sensory Window (a depiction of what information is currently being represented within EPIC's sensory processors), and a Perceptual Window (a depiction of what information is currently represented within EPIC's perceptual processors). There is also the Normal Output window that shows most information about the simulation status, errors, etc. It also shows a continual trace of the current contents of EPIC's Working Memory (updated every 50 ms) while a simulation is running. The Trace Output window shows detailed trace information for various EPIC processors, but only if they are enabled in the settings. By default, no trace information will be displayed. The Stats Output window shows any statistical analysis information written by the device and can include text, tables, and graphs. EPICpy's GUI allows setting various general options via the Settings menu, as well as various model-specific options via the Settings and Run menus. The GUI also facilitates loading Task Devices, Task Rules, and Perceptual Encoders, which together represent a single psychological model for simulation.","title":"EPICpy Application"},{"location":"epicpysimcomponents/#task-devices","text":"The File\u27a1Load_Device menu item is used to load a Task Device into the simulation environment. A task device is a Python code file that represents a simulated performance environment that EPIC (the simulated human performer) will interact with. The device represents the hardware and software that users interact with, and can present visual and auditory stimuli, manual and vocal response devices, and task logic. Thus, the task device controls the progression of a task and dictates when various stimuli are shown and how it handles inputs from EPIC. For example, researchers may run human participants in trials of a task using a computer monitor and speakers to display stimuli and warnings, as well as a keyboard to register responses. Data from this human run would be collected and stored. To understand performance in such a task, researcher might produce a hypothesis based on theory or performance in similar tasks. They may wish to test their hypothesis for how humans performed this task by simulating that performance. The task device would be constructed to mimic the human task device (again, including all relevant inputs and outputs, as well as corresponding data collection). The task device would be programmed to sufficient detail to serve as a reasonable test of the hypothesized performance strategy, but may omit details deemed insignificant. E.g., your hypothesized model for how one uses a word processor may omit representation of the web camera affixed to the top of the monitor if the presence of that camera is presumed to have no significant effect on task performance. Similarly, one might omit texture and shading in a model of driving performance, if these visual details are deemed irrelevant. Of course assumption about what to include, or exclude from the simulated task device could be incorrect, which is why such assumptions are effectively a part of the psychological model being tested. Currently, EPIC (the architecture) allows flexible object-based representations of visual and auditory stimuli (arbitrary sounds and speech) localized in space relative to the simulated performer. EPIC also allows a number of manual response devices (e.g., response-box, keyboard, joystick, mouse movement, etc.), each with multiple response styles. EPIC also supports ocular movements and vocal responses. Thus, these inputs and outputs are the ones available to task devices. For information on how to program EPICpy devices, see Programming Overview section.","title":"Task Devices"},{"location":"epicpysimcomponents/#perceptual-encoders","text":"Although EPICpy does not provide a way for modelers to alter EPIC itself (the architecture which is programmed in C++ and imported into EPICpy as a compiled, shared library), there is a way to write Python code that alters perceptual processing within EPIC. A Perceptual Encoder is a set of Python code that represents a psychological model for how some stimulus features are encoded into EPIC's Working Memory following sensation and early perceptual processes, but prior to Worming Memory representation. For example, if a furniture novice is shown a display containing a number of furniture items, these objects may be encoded into working memory as [\"chair\", \"table\", \"sofa\", \"lamp\"]. A furniture expert may have enough experience that they instead perceive the same visual items as [\"Eames\", \"VanDerRohe\", \"KnollBassett\", \"Arco\"]. This transformation could be embodied in a Visual Encoder. Other visual encoders may make adjustments for participants with red-green color blindness, or add error to object spatial localization due to lack of experience or fatigue. Thus, an encoder translates or alters perceptual information before it is added to EPIC's Working Memory. EPIC also allows Auditory Encoders that do the same kind of transform but for auditory information. For example, an auditory encoder could be used to represent how people mis-hear certain words, or mis-localize certain sounds. Similarly, an auditory encoder can be used to perceive tones of 440, 880, and 1600 Hertz and perceive them as 'high', 'medium', and 'low'. EPICpy allows the simultaneous use of Visual and Auditory encoders. Encoders can be loaded using the File\u27a1Load_Visual_Encoder , and File\u27a1Load_Auditory_Encoder menu items. These items are only accessible after a device has been loaded. On subsequent device loads, EPICpy will automatically re-establish previously used encoders. Encoders can also be unloaded from a simulation using the File\u27a1Unload_Visual_Encoder , and File\u27a1Unload_Auditory_Encoder menu items. For information on how to program EPICpy encoders, see Programming Overview section.","title":"Perceptual Encoders"},{"location":"epicpysimcomponents/#task-rules","text":"Whereas the Task Device simulates the task performance context (inputs, outputs, sequence, data collection and scoring, stopping rules, etc.) the Task Rules represent a strategy for performing the task. These rules can either represent conscious strategies one uses to follow task rules and meet task goals, or non-conscious cognitive processing that occurs during task performance. Task Rules are written generally in a format consistent with other \"Production Rules\" used in other simulation systems. However, the specific syntax and grammar EPIC expects is defined as part of the Parsimonious Production System (PPS) by David Kieras (2004). To learn how to read/write production rules in EPIC, one can read the EPIC Principles of Operation (2004) by David Kieras. Although unnecessary for most modelers, to dive deeper and understand how PPS works you can read PPS: A Parsimonious Production System (1987) by Arie Covrigaru and David Kieras. Task rules are loaded using the File\u27a1Compile_Rules menu item. Rule files are typically plain text files with a .prs file extension. Rule files may contain any number of production rules, each with a simple IF/THEN organization. The IF section contains a set of clauses. If each of these clauses is true for the contents of EPIC's WM when evaluated, the rule will \"fire\", that is, cary out each of the commands in the THEN section. For example: (VIS_visual_Fixation_onset If ( (Goal Do detection_task) (Step Waitfor Visual_Fixation_Onset) (Visual ?a Detection Onset) (Motor Ocular Processor Free) ) Then ( (Send_to_motor Ocular Perform Move ?a) (Add (Tag ?a Fixation_Symbol)) (Delete (Step Waitfor Visual_Fixation_Onset)) (Add (Step Waitfor Visual_Stimulus_Onset)) )) Rule files also contain other information that are not rules, but configuration information for EPIC. E.g.:","title":"Task Rules"},{"location":"epicpysimcomponents/#initial-wm-contents","text":"The modeler may optionally specify some contents to be added to WM prior to simulation start using the Define Initial_memory_contents command. E.g., at the top of the rule file, on might include this line: (Define Initial_memory_contents (Goal Do detection_task))","title":"Initial WM Contents"},{"location":"epicpysimcomponents/#epic-simulation-configuration","text":"In order to configure various aspects of EPIC's motor, sensory, and perceptual system, directives may be included at the top of a production rule file in a Define Parameters clause. E.g.: (Define Parameters # Specify distribution governing visual perceptual recoding rate (Visual_perceptual_processor Recoding_failure_rate1 Fixed 0.5) # Specify distribution governing manual motor accuracy (Manual Execution_fluctuation_factor Normal When_used 1.0 0.1) # Specify distribution visual sensory delays (Eye Property_delay_fluctuation_factor Normal When_used 1.0 0.1) ) Why would you want to alter the way EPIC's processors work when creating a simulation to model performance in a particular task? EPIC (the architecture) is programmed with various default parameters. E.g., perception of text by default occurs without error roughly 100 ms after sensation with very little temporal variation. Imagine this default was based on previous work using 12 point Arial font in black ink on a white display from 40 cm away. This default could be inappropriate for the task device you are using. E.g., a task using 8 point text written in medium gray ink on a light gray background, may result in slower perceptual encoding times, and greater encoding error and may require precise foveation to see. You can reflect this visual reality in the Define Parameters section. On the other extreme, imagine a task with 72 point font written in high contrast. It may be faster and less error-prone to encode such a stimulus. This section can also be used to specify the distribution parameters for availability functions. Availability functions dictate the probability that a perceptual property (e.g., color) will be represented by EPIC's ocular processor as a function of size and foveal eccentricity. For detail on EPIC parameters and availability functions, including how to specify them, see Kieras's EPIC Principles of Operation (2004) .","title":"EPIC Simulation Configuration"},{"location":"epicpysimcomponents/#named-locations","text":"A third common directive modelers place at the top of their rules file sets up Named Locations. These named locations can be referred to in rules in order to, for example, direct eye movements or mouse responses. If you give participants instructions that refer to \"the center of the screen\", \"the menu area\", etc., their strategy to perform the task may be in reference to these named locations. To specify named locations, the Define Named_location directive is used, e.g.: (Define Named_location Center_screen 0.0 0.0)","title":"Named Locations"},{"location":"epicpysimcomponents/#putting-it-all-together","text":"So, an EPIC simulation involves a Task Device that represents a virtual version of a task. On each trial of the task, EPIC's behavior is dictated by the contents of the Task Rules. Operation of EPIC itself can be influenced by some combination of perceptual encoders and parameter directives. EPIC operates on 50 ms cycles; every 50 ms the contents of WM are compared to each rule in the rule file, any and all rules that match the current state of WM will fire and update WM and/or initiate some motor movement. This continues until the task is done, or nothing happens for a predetermined number of cycles (1000 by default). At the end of your simulation, the device would store any relevant data produced by EPIC (e.g., virtual response times, errors, mose movements, keyboard keys pressed, etc.). This virtual data can then be compared to the previously collected human data. If these data match closely, the strategy represented by the rule file is supported. If the data do not match, the hypothesized strategy may be incorrect or incomplete. The approach described above is a post-hoc way of using EPIC (human data first, generate hypothesized strategy after analyzing data, then verify understanding using an EPIC simulation). EPIC can also be used to do predictive modeling whereby simulated data is produced first, and then the generalizability of the corresponding production rules can be assessed by comparing data from later human participants. This might be more useful in a design context. E.g., EPIC may be used to iterate through various cockpit interface changes until a range of reasonable virtual performance is observed. Then a physical mockup of the task device can be created for testing humans and further refinement. Note that whereas task devices always specify stimuli to present to EPIC and response handlers to deal with responses EPIC makes, nothing else is guaranteed. This means that the kind of feedback, data storage, or post-task data analysis that is available is entirely up to the device programmer. Typically, devices represent a number of task trials (sometimes within a block structure), and records and stores trial-by-trial data to disk. In EPICpy, this will be stored in a file called data_output.csv within the same folder in which the device file is located. However, device programmers can decide whether to make use of this file, and may sometimes store other data files. Some devices present basic (often quite minimal) statistical analysis at the end of each run for quick verification of model performance. This analysis may simply indicate mean RT and number of correct responses. Sometimes more involved analyses are done using some basic facilities defined within the EPIC architecture (e.g., RMSE and other simple goodness-of-fit statistics). EPICpy has greatly expanded the type of analyses that can be done without reinventing the wheel by allowing model programmers to use a rich set of statistical and graphing packages available for statistics and graphing in Python. For more detail of the statistical facility in EPICpy, see the section on EPICpy's Stat Window .","title":"Putting It All Together"},{"location":"epicpystats/","text":"EPICpy Statistics Facility Python has a rich set of modules and packages for statistical analysis and graphing. Because EPICpy includes the Pingouin Statistical Package , device programmers have access to the following facilities (see Pingouin docs for other imported packages): pingouin statsmodels pandas pandas-flavor numpy scipy matplotlib seaborn Access to these packages in device code is enabled through standard Python import statements. Display of statistical analyses and graphs is enabled by using the stats_write() method which can accept text, pandas dataframes and other objects, as well as figures and other matplotlib-based objects. Although we don't recommend programming full data analyses at the end of each simulation run, this statistical facility will allow most conceivable post-run analyses a modeler might need.","title":"EPICpy Statistics Facility"},{"location":"epicpystats/#epicpy-statistics-facility","text":"Python has a rich set of modules and packages for statistical analysis and graphing. Because EPICpy includes the Pingouin Statistical Package , device programmers have access to the following facilities (see Pingouin docs for other imported packages): pingouin statsmodels pandas pandas-flavor numpy scipy matplotlib seaborn Access to these packages in device code is enabled through standard Python import statements. Display of statistical analyses and graphs is enabled by using the stats_write() method which can accept text, pandas dataframes and other objects, as well as figures and other matplotlib-based objects. Although we don't recommend programming full data analyses at the end of each simulation run, this statistical facility will allow most conceivable post-run analyses a modeler might need.","title":"EPICpy Statistics Facility"},{"location":"epicresources/","text":"EPIC Articles and other Resources Original EPIC Papers: Architectural Overview and Application Papers Meyer, D. E., & Kieras, D. E. (1997). A computational theory of executive control processes and human multiple-task performance: Part 1. Basic Mechanisms. Psychological Review, 104, 3-65. Meyer, D. E., & Kieras, D. E. (1997). A computational theory of executive cognitive processes and multiple-task performance: Part 2. Accounts of psychological refractory-period phenomena. Psychological Review, 104, 749-791. Details of EPIC Internals Kieras, D. (2004). EPIC Architecture Principles of Operation. Various Published Uses of EPIC Kieras, D. E. (2019). Visual Search Without Selective Attention: A Cognitive Architecture Account. Topics in Cognitive Science, 11, (pp. 222-239). Kieras, D. E., Hornoff, A. J. (2014). Towards Accurate and Practical Predictive Models of Active-Vision-Based Visual Search. Proceedings of The CHI2014 Conference on Human Factors in Computing Systems (pp. 3875-3884). Kieras, D. (2010). Modeling Visual Search of Displays of Many Objects: The Role of Differential Acuity and Fixation Memory. Proceedings of the 10th International Conference on Cognitive Modeling, ICCM 2010. Kieras, D. E., Meyer, D. E., & Ballas, J. (2001). Towards demystification of direct manipulation: Cognitive modeling charts the gulf of execution. Proceedings of The CHI2001 Conference on Human Factors in Computing Systems (pp. 128-135). New York: Association of Computing Machinery, 2001. Kieras, D. E., Meyer, D. E., Ballas, J. A., & Lauber, E. J. (2000). Modern Computational Perspectives on Executive Mental Processes and Cognitive Control: Where to from Here?. In S. Monsell & J. Driver (eds.) Control of Cognitive Processes: Attention and Performance XVIII, (pp. 681-712). Cambridge, MA: M.I.T. Press, 2000. Kieras, D. & Meyer, D.E. (1997). An overview of the EPIC architecture for cognition and performance with application to human-computer interaction. Human-Computer Interaction, 12, 391-438. Kieras, D.E., Wood, S.D., & Meyer, D.E. (1997). Predictive engineering models based on the EPIC architecture for a multimodal high-performance human-computer interaction task. ACM Transactions on Computer-Human Interaction, 4, 230-275. Meyer, D.E., Kieras, D.E., Lauber, E., Schumacher, E.H., Glass, J., Zurbriggen, E., Gmeindl, L., & Apfelblat, D. (1995). Adaptive executive control: Flexible multiple-task performance without pervasive immutable response-selection bottlenecks. Acta Psychologica, 90, 163-190. Kieras, D.E., Meyer, D.E., Mueller, S., & Seymour, T. (1999). Insights into working memory from the perspective of the EPIC architecture for modeling skilled perceptual-motor and cognitive human performance. In A. Miyake and P. Shah (Eds.), Models of Working Memory: Mechanisms of Active Maintenance and Executive Control. New York: Cambridge University Press. 183-223. Meyer, D. E., & Kieras, D. E. (1999). Precis to a practical unified theory of cognition and action: Some lessons from computational modeling of human multiple-task performance. In D. Gopher & A. Koriat (Eds.), Attention and Performance XVII. Cognitive regulation of performance: Interation of theory and application (pp. 17 -88). Cambridge, MA: M.I.T. Press. Kieras, D.E., Meyer, D.E., Ballas, J.A., Lauber, E.J. Modern computational perspectives on executive mental processes and cognitive control. Where to from here? (EPIC Tech. Rep. No. 12, TR-98/ONR-EPIC-12). Ann Arbor, University of Michigan, Electrical Engineering and Computer Science Department. August 1, 1999. Kieras, D. E. & Meyer, D.E. An overview of the EPIC architecture for cognition and performance with application to human-computer interaction. (EPIC Tech. Rep. No. 5, TR-95/ONR-EPIC-5). Ann Arbor, University of Michigan, Electrical Engineering and Computer Science Department. December 5, 1995. Kieras, D.E., Wood, S.D., & Meyer, D.E. Predictive engineering models based on the EPIC architecture for a multimodal high-performance human-computer interaction task. (EPIC Tech. Rep. No. 4, TR-95/ONR-EPIC-4). Ann Arbor, University of Michigan, Electrical Engineering and Computer Science Department. October 1, 1995. Kieras, D.E., & Meyer, D.E. (1994). The EPIC architecture for modeling human information-processing: A brief introduction. (EPIC Tech. Rep. No. 1, TR-94/ONR-EPIC-1). Ann Arbor, University of Michigan, Department of Electrical Engineering and Computer Science. Kieras, D.E., & Meyer, D.E. (1995). Predicting performance in dual-task tracking and decision making with EPIC computational models. Proceedings of the First International Symposium on Command and Control Research and Technology, National Defense University, Washington, D.C., June 19-22. 314-325. Hornof, A. J. & Kieras, D. E. (1997). Cognitive modeling reveals menu search is both random and systematic. Proceedings of the CHI'97 Conference on Human Factors in Computing Systems, 107-114. New York: ACM. Seymour, T.L., Schumacher, E.H., (2009). Electromyographic Evidence for Response Conflict in the Exclude Recognition Task. Cognitive, Affective & Behavioral Neuroscience, 9(1), 71-82.","title":"EPIC Resources"},{"location":"epicresources/#epic-articles-and-other-resources","text":"","title":"EPIC Articles and other Resources"},{"location":"epicresources/#original-epic-papers-architectural-overview-and-application-papers","text":"Meyer, D. E., & Kieras, D. E. (1997). A computational theory of executive control processes and human multiple-task performance: Part 1. Basic Mechanisms. Psychological Review, 104, 3-65. Meyer, D. E., & Kieras, D. E. (1997). A computational theory of executive cognitive processes and multiple-task performance: Part 2. Accounts of psychological refractory-period phenomena. Psychological Review, 104, 749-791.","title":"Original EPIC Papers: Architectural Overview and Application Papers"},{"location":"epicresources/#details-of-epic-internals","text":"Kieras, D. (2004). EPIC Architecture Principles of Operation.","title":"Details of EPIC Internals"},{"location":"epicresources/#various-published-uses-of-epic","text":"Kieras, D. E. (2019). Visual Search Without Selective Attention: A Cognitive Architecture Account. Topics in Cognitive Science, 11, (pp. 222-239). Kieras, D. E., Hornoff, A. J. (2014). Towards Accurate and Practical Predictive Models of Active-Vision-Based Visual Search. Proceedings of The CHI2014 Conference on Human Factors in Computing Systems (pp. 3875-3884). Kieras, D. (2010). Modeling Visual Search of Displays of Many Objects: The Role of Differential Acuity and Fixation Memory. Proceedings of the 10th International Conference on Cognitive Modeling, ICCM 2010. Kieras, D. E., Meyer, D. E., & Ballas, J. (2001). Towards demystification of direct manipulation: Cognitive modeling charts the gulf of execution. Proceedings of The CHI2001 Conference on Human Factors in Computing Systems (pp. 128-135). New York: Association of Computing Machinery, 2001. Kieras, D. E., Meyer, D. E., Ballas, J. A., & Lauber, E. J. (2000). Modern Computational Perspectives on Executive Mental Processes and Cognitive Control: Where to from Here?. In S. Monsell & J. Driver (eds.) Control of Cognitive Processes: Attention and Performance XVIII, (pp. 681-712). Cambridge, MA: M.I.T. Press, 2000. Kieras, D. & Meyer, D.E. (1997). An overview of the EPIC architecture for cognition and performance with application to human-computer interaction. Human-Computer Interaction, 12, 391-438. Kieras, D.E., Wood, S.D., & Meyer, D.E. (1997). Predictive engineering models based on the EPIC architecture for a multimodal high-performance human-computer interaction task. ACM Transactions on Computer-Human Interaction, 4, 230-275. Meyer, D.E., Kieras, D.E., Lauber, E., Schumacher, E.H., Glass, J., Zurbriggen, E., Gmeindl, L., & Apfelblat, D. (1995). Adaptive executive control: Flexible multiple-task performance without pervasive immutable response-selection bottlenecks. Acta Psychologica, 90, 163-190. Kieras, D.E., Meyer, D.E., Mueller, S., & Seymour, T. (1999). Insights into working memory from the perspective of the EPIC architecture for modeling skilled perceptual-motor and cognitive human performance. In A. Miyake and P. Shah (Eds.), Models of Working Memory: Mechanisms of Active Maintenance and Executive Control. New York: Cambridge University Press. 183-223. Meyer, D. E., & Kieras, D. E. (1999). Precis to a practical unified theory of cognition and action: Some lessons from computational modeling of human multiple-task performance. In D. Gopher & A. Koriat (Eds.), Attention and Performance XVII. Cognitive regulation of performance: Interation of theory and application (pp. 17 -88). Cambridge, MA: M.I.T. Press. Kieras, D.E., Meyer, D.E., Ballas, J.A., Lauber, E.J. Modern computational perspectives on executive mental processes and cognitive control. Where to from here? (EPIC Tech. Rep. No. 12, TR-98/ONR-EPIC-12). Ann Arbor, University of Michigan, Electrical Engineering and Computer Science Department. August 1, 1999. Kieras, D. E. & Meyer, D.E. An overview of the EPIC architecture for cognition and performance with application to human-computer interaction. (EPIC Tech. Rep. No. 5, TR-95/ONR-EPIC-5). Ann Arbor, University of Michigan, Electrical Engineering and Computer Science Department. December 5, 1995. Kieras, D.E., Wood, S.D., & Meyer, D.E. Predictive engineering models based on the EPIC architecture for a multimodal high-performance human-computer interaction task. (EPIC Tech. Rep. No. 4, TR-95/ONR-EPIC-4). Ann Arbor, University of Michigan, Electrical Engineering and Computer Science Department. October 1, 1995. Kieras, D.E., & Meyer, D.E. (1994). The EPIC architecture for modeling human information-processing: A brief introduction. (EPIC Tech. Rep. No. 1, TR-94/ONR-EPIC-1). Ann Arbor, University of Michigan, Department of Electrical Engineering and Computer Science. Kieras, D.E., & Meyer, D.E. (1995). Predicting performance in dual-task tracking and decision making with EPIC computational models. Proceedings of the First International Symposium on Command and Control Research and Technology, National Defense University, Washington, D.C., June 19-22. 314-325. Hornof, A. J. & Kieras, D. E. (1997). Cognitive modeling reveals menu search is both random and systematic. Proceedings of the CHI'97 Conference on Human Factors in Computing Systems, 107-114. New York: ACM. Seymour, T.L., Schumacher, E.H., (2009). Electromyographic Evidence for Response Conflict in the Exclude Recognition Task. Cognitive, Affective & Behavioral Neuroscience, 9(1), 71-82.","title":"Various Published Uses of EPIC"},{"location":"example/","text":"EPICpy Simulation Example EPICpy comes with 2 sample devices, a Donders-style Choice Task , and a device based on the Experiment 5 paradigm from Findlay (1997) . Please note that these devices are for demonstration purposes only and the production rules provided may not represent serious psychological models of human performance (especially the Choice Task device). Consider the Choice Task. The Choice Task Overview This task asks participants to perform many trials. On each trial, a central warning symbol will be shown for 1000 ms, in this case the octothorpe/number-sign/hash symbol (#). This warning is used to indicate that the stimulus will be displayed next, and to encourage a central ocular fixation. Next the screen is blank for a randomly chosen duration between 500 and 1300 ms. After this period, a 2 o diameter colored circle (the stimulus) is shown centered vertically on the display. On each trial, the horizontal eccentricity can vary randomly according to a uniform distribution. The horizontal offsets will either be -2 o , -1 o , 0 o , 1 o , or 1 o relative to the center of the display. Note that this is the \" Hard \" condition. There is also an \" Easy \" condition where the stimulus is always displayed in the center of the screen. Stimulus color is randomly chosen each trial from the color set, which can be set between 1 and 4. The full set of task colors is Red, Green, Blue and Yellow. Responses for Red, Green, Blue, and Yellow stimuli are made by pressing the U, I, O, and P keys on the keyboard using the Index, Middle, Ring, and Little fingers on participants' right hand, respectively. After the stimulus is presented, the task waits until a response is made before continuing. On each trial, response time (in milliseconds) and response accuracy are recorded and saved to disk. Between trials there is a 2500 ms delay before starting the next trial. Choice Task Events Human Data The figure below depicts some data one might observe when testing humans in the choice task described above (\"Easy\" condition, i.e., stimulus is always presented at screen center): Choice Task Human Data Based on the figure above, we can conclude that participants can generally make these choice responses in around half a second, with red being a little faster and green being a little slower than the others. Why is this? We'll come back to this question later. For now, let's consider a logically minimal strategy for this task in the form of a set of production rules. An EPIC Model of the Human Data The choicetask_rules_VM.prs file include with the choice task demo device, and located in the rules subfolder. These rules embody a strategy for how participants might perform this task based solely on what is minimally required to follow task instructions: Participants first wait for a visual object to appear at display center, at which point they make a mental note that this object must be the fixation symbol they were expecting. They then begin waiting for the stimulus to arrive. Participants wait for a different visual object to appear at display center, at which point they look at it and make a mental note that this object must be the stimulus they were expecting. They then begin waiting to become aware of the object's color. Participants wait for the object's color, when this information arrives in WM (i.e., they become consciously aware of it), they make a note that they need to press the key associated with this particular color Assuming their hands are not currently busy doing anything else, participants subsequently plan a motor movement on their right hand corresponding to the color-key mapping they learned during the instructions. For example, if the stimulus is Red, they will initiate a movement with their right index finger to press the U key on the keyboard. The entire mapping is the keys UIOP for the colors RGBY, respectively. Once the manual response has been initiated, participants begin waiting for the next trial to begin. The following is a process model for the minimal ruleset: Initial Model Process Diagram To run this model and see how well it fits (i.e., matches) our human data, we need to do the following: Start EPICpy Use File\u27a1Load_Device to load the Choice Task device ( choice_device.py ) Use File\u27a1Compile_Rules load the Choice Task Visual-Manual ruleset ( choicetask_rules_VM ) 4.Press Run\u27a1Run_Settings to set up the device condition string to match the conditions under which our human data were collected. This would require the string \"80 4 Easy [r1|r2|r3|r4|r5|r6|r7|r8|r9|r10]\" , which indicates 80 trials per simulated run, a maximum of 4 possible colors, the Easy condition (stimulus always in screen center), and 10 runs named r1...r10 in the datafile. On the Run_Settings dialog, press the Delete_Current_Device_Datafile button to clear the data file (in case there already some data from a previous run). Use Run\u27a1Run to start the simulation (this will take some time to run, even on faster machines unless you disable put the RUN_SETTINGS in fast-mode (only update text output and view windows after the run finishes)). The result of this simulation are compared to the human data in the following figure: First Run Comparison of Human and Simulated Data According to this graph, the simulated data appears to represent significantly faster responses than observed in human subjects. Furthermore, it appears that stimulus color is less affected in the simulated data than in the human data. In other words, the fitness (aka \"fit\") between the human and simulated data is quite poor. This likely means that the performance strategy represented by our production rules is a poor model for the way humans performed the task. Coming Up With A Better Model At list point, let's consider a more psychologically plausible theory for choice task performance: Modified Model Process Diagram The default rules provided with the choicetask are just a minimal strategy, and not intended to be psychological realistic. Here we add 2 important changes: 1. Perceptual Rule Changes : The human data are from humans with only minimal practice, thus it is reasonable to assume that stimulus-response mapping is still in the declarative stage, and not yet automatized. To implement this, we will edit the rules so that response-selection and response-execution are separate stages. For example, we might make these changes to the rules: Original Perpetual Rules New Perpetual Rules 2. Motor Rule Changes : Although the stimulus shape in this task is not required for accurate responses, relatively unpracticed participants may use the stimulus shape to distinguish it from the fading working memory trace of the fixation symbol. Thus, here we assume that participants wait to perceive the stimulus shape before considering its color. Original Perpetual Rules New Perpetual Rules You can edit the currently loaded ruleset in any text editor. Right-clicking on the Normal Output window and selecting \" Edit Production Rule File \" will open the currently selected rules in your system's default editor for .prs text files. Once you are finished editing the ruleset and have saved it to disk (don't forget this step), use File\u27a1Recompile_Rules . Alternatively, you could choose File\u27a1Reload_Last_Session , which would reload both the device and the ruleset. Note that the data for the previous run is still within data_output.csv, so we need to clear out the data file in the Run_Settings dialog. Finally, we run the new model to see the modified fit using Run\u27a1Run . The new model's fit between simulated EPIC data and Human data is shown below: The fit is better, but still not quite right. What's Next? Clearly, the minimal model is not sufficient, but our modifications are still not capturing the observed human performance. One thing you may have noticed in the task description is that there is a static mapping between the colors Red, Green, Blue, Yellow, and the keys U, I, O, and P. These keys are likewise statically assigned to the fingers Index, Middle, Ring, and Little. This means that there is a confound between color and finger. Although fine for a toy device, a serious psychological experiment would have counterbalanced these associations. In any case, this means that there are two possible explanations for the differences between colors observed in the human data. For example, why is the mean response to Red faster than for other colors? It is possible that humans are faster to perceive red than the other colors. It is also possible that humans are faster to push buttons with their index finger than other fingers. Such assumptions can be added to the model using a visual encoder (e.g., to allow faster perception of Red than other colors) or by altering the rules (e.g., by pre-preparing the Red response at the start of each trial to represent a response-bias). As this is the documentation for the EPICpy application, not EPIC modeling itself, one will have to look elsewhere to learn how to create/edit rulesets, write encoders, and generally explore the model space for a given task.","title":"Example Simulation"},{"location":"example/#epicpy-simulation-example","text":"EPICpy comes with 2 sample devices, a Donders-style Choice Task , and a device based on the Experiment 5 paradigm from Findlay (1997) . Please note that these devices are for demonstration purposes only and the production rules provided may not represent serious psychological models of human performance (especially the Choice Task device). Consider the Choice Task.","title":"EPICpy Simulation Example"},{"location":"example/#the-choice-task","text":"","title":"The Choice Task"},{"location":"example/#overview","text":"This task asks participants to perform many trials. On each trial, a central warning symbol will be shown for 1000 ms, in this case the octothorpe/number-sign/hash symbol (#). This warning is used to indicate that the stimulus will be displayed next, and to encourage a central ocular fixation. Next the screen is blank for a randomly chosen duration between 500 and 1300 ms. After this period, a 2 o diameter colored circle (the stimulus) is shown centered vertically on the display. On each trial, the horizontal eccentricity can vary randomly according to a uniform distribution. The horizontal offsets will either be -2 o , -1 o , 0 o , 1 o , or 1 o relative to the center of the display. Note that this is the \" Hard \" condition. There is also an \" Easy \" condition where the stimulus is always displayed in the center of the screen. Stimulus color is randomly chosen each trial from the color set, which can be set between 1 and 4. The full set of task colors is Red, Green, Blue and Yellow. Responses for Red, Green, Blue, and Yellow stimuli are made by pressing the U, I, O, and P keys on the keyboard using the Index, Middle, Ring, and Little fingers on participants' right hand, respectively. After the stimulus is presented, the task waits until a response is made before continuing. On each trial, response time (in milliseconds) and response accuracy are recorded and saved to disk. Between trials there is a 2500 ms delay before starting the next trial. Choice Task Events","title":"Overview"},{"location":"example/#human-data","text":"The figure below depicts some data one might observe when testing humans in the choice task described above (\"Easy\" condition, i.e., stimulus is always presented at screen center): Choice Task Human Data Based on the figure above, we can conclude that participants can generally make these choice responses in around half a second, with red being a little faster and green being a little slower than the others. Why is this? We'll come back to this question later. For now, let's consider a logically minimal strategy for this task in the form of a set of production rules.","title":"Human Data"},{"location":"example/#an-epic-model-of-the-human-data","text":"The choicetask_rules_VM.prs file include with the choice task demo device, and located in the rules subfolder. These rules embody a strategy for how participants might perform this task based solely on what is minimally required to follow task instructions: Participants first wait for a visual object to appear at display center, at which point they make a mental note that this object must be the fixation symbol they were expecting. They then begin waiting for the stimulus to arrive. Participants wait for a different visual object to appear at display center, at which point they look at it and make a mental note that this object must be the stimulus they were expecting. They then begin waiting to become aware of the object's color. Participants wait for the object's color, when this information arrives in WM (i.e., they become consciously aware of it), they make a note that they need to press the key associated with this particular color Assuming their hands are not currently busy doing anything else, participants subsequently plan a motor movement on their right hand corresponding to the color-key mapping they learned during the instructions. For example, if the stimulus is Red, they will initiate a movement with their right index finger to press the U key on the keyboard. The entire mapping is the keys UIOP for the colors RGBY, respectively. Once the manual response has been initiated, participants begin waiting for the next trial to begin. The following is a process model for the minimal ruleset: Initial Model Process Diagram To run this model and see how well it fits (i.e., matches) our human data, we need to do the following: Start EPICpy Use File\u27a1Load_Device to load the Choice Task device ( choice_device.py ) Use File\u27a1Compile_Rules load the Choice Task Visual-Manual ruleset ( choicetask_rules_VM ) 4.Press Run\u27a1Run_Settings to set up the device condition string to match the conditions under which our human data were collected. This would require the string \"80 4 Easy [r1|r2|r3|r4|r5|r6|r7|r8|r9|r10]\" , which indicates 80 trials per simulated run, a maximum of 4 possible colors, the Easy condition (stimulus always in screen center), and 10 runs named r1...r10 in the datafile. On the Run_Settings dialog, press the Delete_Current_Device_Datafile button to clear the data file (in case there already some data from a previous run). Use Run\u27a1Run to start the simulation (this will take some time to run, even on faster machines unless you disable put the RUN_SETTINGS in fast-mode (only update text output and view windows after the run finishes)). The result of this simulation are compared to the human data in the following figure: First Run Comparison of Human and Simulated Data According to this graph, the simulated data appears to represent significantly faster responses than observed in human subjects. Furthermore, it appears that stimulus color is less affected in the simulated data than in the human data. In other words, the fitness (aka \"fit\") between the human and simulated data is quite poor. This likely means that the performance strategy represented by our production rules is a poor model for the way humans performed the task.","title":"An EPIC Model of the Human Data"},{"location":"example/#coming-up-with-a-better-model","text":"At list point, let's consider a more psychologically plausible theory for choice task performance: Modified Model Process Diagram The default rules provided with the choicetask are just a minimal strategy, and not intended to be psychological realistic. Here we add 2 important changes: 1. Perceptual Rule Changes : The human data are from humans with only minimal practice, thus it is reasonable to assume that stimulus-response mapping is still in the declarative stage, and not yet automatized. To implement this, we will edit the rules so that response-selection and response-execution are separate stages. For example, we might make these changes to the rules: Original Perpetual Rules New Perpetual Rules 2. Motor Rule Changes : Although the stimulus shape in this task is not required for accurate responses, relatively unpracticed participants may use the stimulus shape to distinguish it from the fading working memory trace of the fixation symbol. Thus, here we assume that participants wait to perceive the stimulus shape before considering its color. Original Perpetual Rules New Perpetual Rules You can edit the currently loaded ruleset in any text editor. Right-clicking on the Normal Output window and selecting \" Edit Production Rule File \" will open the currently selected rules in your system's default editor for .prs text files. Once you are finished editing the ruleset and have saved it to disk (don't forget this step), use File\u27a1Recompile_Rules . Alternatively, you could choose File\u27a1Reload_Last_Session , which would reload both the device and the ruleset. Note that the data for the previous run is still within data_output.csv, so we need to clear out the data file in the Run_Settings dialog. Finally, we run the new model to see the modified fit using Run\u27a1Run . The new model's fit between simulated EPIC data and Human data is shown below: The fit is better, but still not quite right.","title":"Coming Up With A Better Model"},{"location":"example/#whats-next","text":"Clearly, the minimal model is not sufficient, but our modifications are still not capturing the observed human performance. One thing you may have noticed in the task description is that there is a static mapping between the colors Red, Green, Blue, Yellow, and the keys U, I, O, and P. These keys are likewise statically assigned to the fingers Index, Middle, Ring, and Little. This means that there is a confound between color and finger. Although fine for a toy device, a serious psychological experiment would have counterbalanced these associations. In any case, this means that there are two possible explanations for the differences between colors observed in the human data. For example, why is the mean response to Red faster than for other colors? It is possible that humans are faster to perceive red than the other colors. It is also possible that humans are faster to push buttons with their index finger than other fingers. Such assumptions can be added to the model using a visual encoder (e.g., to allow faster perception of Red than other colors) or by altering the rules (e.g., by pre-preparing the Red response at the start of each trial to represent a response-bias). As this is the documentation for the EPICpy application, not EPIC modeling itself, one will have to look elsewhere to learn how to create/edit rulesets, write encoders, and generally explore the model space for a given task.","title":"What's Next?"},{"location":"faq/","text":"EPICpy Frequently Asked Questions General How do I Set Up Syntax-Highlighting for EPIC Production Rule Files? Linux Specific Windows Specific MacOS Specific","title":"Frequently Asked Questions"},{"location":"faq/#epicpy-frequently-asked-questions","text":"","title":"EPICpy Frequently Asked Questions"},{"location":"faq/#general","text":"How do I Set Up Syntax-Highlighting for EPIC Production Rule Files?","title":"General"},{"location":"faq/#linux-specific","text":"","title":"Linux Specific"},{"location":"faq/#windows-specific","text":"","title":"Windows Specific"},{"location":"faq/#macos-specific","text":"","title":"MacOS Specific"},{"location":"installing/","text":"Installing EPICpy (and epiccoder rule editor) Operating System Requirements Although we have not tested widely, the following setups should work: Windows 10 and Windows 11 MacOS Monteray and later (Intel and ARM based chips) Linux with Ubuntu based distros version 20.10 and later Linux versions earlier than 20.10 could work, but ldd --version would have to print some version >= 2.32, and you might end up having to install several other libraries. Installing Prerequisites Installing these applications requires the following utilities to be installed on your computer: curl git uv Python 3.10 (MacOS and Linux) or Python 3.9 (Windows) Installing curl curl is installed by default on MacOS and Windows. Some Linux distributions come with curl , and others (especially \"minimal\" distributions) may not. If this check reports that curl is not installed: curl --version then you can install curl with your package manager, e.g.: sudo apt install curl Installing git git is not typically installed by default. If this check reports that git is not installed: git --version then you may need to install git . In all most cases, you can go here: https://git-scm.com/downloads and use the installer that goes with your operating system. If you prefer a command-line solutions: MacOS: Install homebrew . run brew install git Windows: Install scoop run scoop install git Linux: run sudo apt install git (replace apt install with whatever works on your system) Installing uv uv is not automatically installed on any operating system. If this check reports that uv is not installed: uv --version Then you can see the installation instructions for on the uv installation webpage , which currently suggests use these commands: MacOS and Linux curl -LsSf https://astral.sh/uv/install.sh | sh Windows powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\" Installing python Your operating system may already have one or more version of Python installed, or it may have none. Assuming you have uv installed, try this command: uv python list --only-installed Here is what I get on my Linux system: cpython-3.12.7-linux-x86_64-gnu /home/user/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/bin/python3 -> python3.12 cpython-3.11.10-linux-x86_64-gnu /home/user/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/bin/python3 -> python3.11 cpython-3.10.12-linux-x86_64-gnu /usr/bin/python3.10 cpython-3.10.12-linux-x86_64-gnu /usr/bin/python3 -> python3.10 cpython-3.10.12-linux-x86_64-gnu /bin/python3.10 cpython-3.10.12-linux-x86_64-gnu /bin/python3 -> python3.10 cpython-3.9.20-linux-x86_64-gnu /home/user/.local/share/uv/python/cpython-3.9.20-linux-x86_64-gnu/bin/python3 -> python3.9 If you are running Linux or MacOS and you see \"3.10\" anywhere in the output, then you have a version of Python (i.e., Python 3.10) that you need to install EPICpy. If you are running Windows and you see \"3.9\" anywhere in the output, then you have a version of Python (i.e., Python 3.10) that you need to install EPICpy. If you don't see the version of Python you need, then it is easy to install it if you have uv installed: MacOS & Linux uv python install 3.10 Windows uv python install 3.9 Downloading Sample Simulations To download the sample simulation specifications ( task devices , associated rules , etc.), you can \"clone\" (or download) them from Prof. Seymour's UCSC git repository: git clone https://git.ucsc.edu/nogard/mhpfiles This will create a folder called mhpfiles . You can see what is in this folder like this: cd mhpfiles # Linux & MacOS ls # Windows dir # ls will also work if you are using powershell Downloading EPICpy and epiccoder [EASY Approach] I have created a python program that you can use to install EPICpy and epiccoder easily if you have uv installed. NOTE: If you are wisely not in the habit or running random code files, feel free to inspect tool_install.py before taking this next step to ensure that there are no malicious steps. If you want some assurance, consider pasting the code into a LLM and asking it what the code does (or whatever approach you'd like to do to reassure yourself). Simply running this command should work on MacOS, Linux, and Windows: uv run tool_install.py If it fails, it will try to tell you what prerequisite you are missing. Although it requires that you have at least installed uv and have some version of Python installed. To ensure that EPICpy correctly installed, try running it: EPICpy If you see the EPICpy graphical interface, it worked! To ensure that epiccoder correctly installed, try running it: epiccoder If you see the epiccoder graphical interface, it worked! Downloading EPICpy and epiccoder [Manual Approach] To install EPICpy and epiccoder manually, first make sure you have successfully installed curl, git, uv, and either Python3.9 (if you run Windows), or Python3.10 (if you run Linux or MacOs). Next, you need to do the following: Obtain the path to python uv python list --only-installed Locate one of the lines containing \"3.9\" (Windows) or \"3.10\" (MacOS & Linux). For example, from this: cpython-3.12.7-linux-x86_64-gnu /home/nogard/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/bin/python3 -> python3.12 cpython-3.11.10-linux-x86_64-gnu /home/nogard/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/bin/python3 -> python3.11 cpython-3.10.12-linux-x86_64-gnu /usr/bin/python3.10 cpython-3.10.12-linux-x86_64-gnu /usr/bin/python3 -> python3.10 cpython-3.10.12-linux-x86_64-gnu /bin/python3.10 cpython-3.10.12-linux-x86_64-gnu /bin/python3 -> python3.10 cpython-3.9.20-linux-x86_64-gnu /home/nogard/.local/share/uv/python/cpython-3.9.20-linux-x86_64-gnu/bin/python3 -> python3.9 The structure of these entries is this: [NAME] [PATH] or [NAME] [PATH] -> [ALIAS] You want to copy the path. So if I choose cpython-3.9.20-linux-x86_64-gnu /home/nogard/.local/share/uv/python/cpython-3.9.20-linux-x86_64-gnu/bin/python3 -> python3.9 I would copy just /home/nogard/.local/share/uv/python/cpython-3.9.20-linux-x86_64-gnu/bin/python3 If I choose cpython-3.10.12-linux-x86_64-gnu /usr/bin/python3.10 then I would copy just /usr/bin/python3.10 Install EPICpy Because EPICpy has specific requirements for which Python version you use on each operating system, you have to include the appropriate python path in the installation command. Generically, the syntax would be like this: uv tool install git+https://www.github.com/travisseymour/EPICpy.git --python [YOUR PYTHON PATH] Specifically, you might enter something like this: # Linux example uv tool install git+https://www.github.com/travisseymour/EPICpy.git --python /usr/bin/python3.10 # MacOS example uv tool install git+https://www.github.com/travisseymour/EPICpy.git --python .local/share/uv/python/cpython-3.10.14-macos-x86_64-none/bin/python3 # Windows example uv tool install git+https://www.github.com/travisseymour/EPICpy.git --python AppData\\Local\\Programs\\Python39\\python.exe Of course, your python path may be different than these examples. Use the path you copied above as a result of running uv python list --only-installed . Install epiccoder epiccoder is less picky about which Python version you use. To install it, just run this: uv tool install git+https://www.github.com/travisseymour/epiccoder.git Testing Your Installations To ensure that EPICpy correctly installed, try running it: EPICpy If you see the EPICpy graphical interface, it worked! To ensure that epiccoder correctly installed, try running it: epiccoder If you see the epiccoder graphical interface, it worked! Running EPIC Simulations IMPORTANT : If you just installed EPICpy and epiccoder , then you have the latest version. However, if you installed EPICpy or epiccoder on your own machine during a previous session, you want to make sure you check for updates before running these tools. uv tool upgrade EPICpy epiccoder or use this script: uv run update_tools.py Now, run a simulation: To start EPICpy, just type EPICpy in a terminal. If this is your first time running EPICpy and the graphical interface seems wonky, then go to the menu and select Windows\ud83e\udc46RestoreDefaultLayout To load a model, you'll need to do: File\ud83e\udc46LoadDevice and then locate your device file. A device file is a python file like choice_device.py File\ud83e\udc46CompileRules and then locate your rules file. A rules file is a text file like choicetask_rules_VM.prs To run a model, you'll need to do: Run\ud83e\udc46Run Try to run the Choice task simulation with the \"VM\" rules 3 times in a row. If you do not see the colorful graph at the end, you may not have the Stats Window open. In the menu, select Windows\ud83e\udc46ShowStatsWindow .","title":"Installing EPICpy"},{"location":"installing/#installing-epicpy-and-epiccoder-rule-editor","text":"","title":"Installing EPICpy (and epiccoder rule editor)"},{"location":"installing/#operating-system-requirements","text":"Although we have not tested widely, the following setups should work: Windows 10 and Windows 11 MacOS Monteray and later (Intel and ARM based chips) Linux with Ubuntu based distros version 20.10 and later Linux versions earlier than 20.10 could work, but ldd --version would have to print some version >= 2.32, and you might end up having to install several other libraries.","title":"Operating System Requirements"},{"location":"installing/#installing-prerequisites","text":"Installing these applications requires the following utilities to be installed on your computer: curl git uv Python 3.10 (MacOS and Linux) or Python 3.9 (Windows) Installing curl curl is installed by default on MacOS and Windows. Some Linux distributions come with curl , and others (especially \"minimal\" distributions) may not. If this check reports that curl is not installed: curl --version then you can install curl with your package manager, e.g.: sudo apt install curl Installing git git is not typically installed by default. If this check reports that git is not installed: git --version then you may need to install git . In all most cases, you can go here: https://git-scm.com/downloads and use the installer that goes with your operating system. If you prefer a command-line solutions: MacOS: Install homebrew . run brew install git Windows: Install scoop run scoop install git Linux: run sudo apt install git (replace apt install with whatever works on your system) Installing uv uv is not automatically installed on any operating system. If this check reports that uv is not installed: uv --version Then you can see the installation instructions for on the uv installation webpage , which currently suggests use these commands: MacOS and Linux curl -LsSf https://astral.sh/uv/install.sh | sh Windows powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\" Installing python Your operating system may already have one or more version of Python installed, or it may have none. Assuming you have uv installed, try this command: uv python list --only-installed Here is what I get on my Linux system: cpython-3.12.7-linux-x86_64-gnu /home/user/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/bin/python3 -> python3.12 cpython-3.11.10-linux-x86_64-gnu /home/user/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/bin/python3 -> python3.11 cpython-3.10.12-linux-x86_64-gnu /usr/bin/python3.10 cpython-3.10.12-linux-x86_64-gnu /usr/bin/python3 -> python3.10 cpython-3.10.12-linux-x86_64-gnu /bin/python3.10 cpython-3.10.12-linux-x86_64-gnu /bin/python3 -> python3.10 cpython-3.9.20-linux-x86_64-gnu /home/user/.local/share/uv/python/cpython-3.9.20-linux-x86_64-gnu/bin/python3 -> python3.9 If you are running Linux or MacOS and you see \"3.10\" anywhere in the output, then you have a version of Python (i.e., Python 3.10) that you need to install EPICpy. If you are running Windows and you see \"3.9\" anywhere in the output, then you have a version of Python (i.e., Python 3.10) that you need to install EPICpy. If you don't see the version of Python you need, then it is easy to install it if you have uv installed: MacOS & Linux uv python install 3.10 Windows uv python install 3.9","title":"Installing Prerequisites"},{"location":"installing/#downloading-sample-simulations","text":"To download the sample simulation specifications ( task devices , associated rules , etc.), you can \"clone\" (or download) them from Prof. Seymour's UCSC git repository: git clone https://git.ucsc.edu/nogard/mhpfiles This will create a folder called mhpfiles . You can see what is in this folder like this: cd mhpfiles # Linux & MacOS ls # Windows dir # ls will also work if you are using powershell","title":"Downloading Sample Simulations"},{"location":"installing/#downloading-epicpy-and-epiccoder-easy-approach","text":"I have created a python program that you can use to install EPICpy and epiccoder easily if you have uv installed. NOTE: If you are wisely not in the habit or running random code files, feel free to inspect tool_install.py before taking this next step to ensure that there are no malicious steps. If you want some assurance, consider pasting the code into a LLM and asking it what the code does (or whatever approach you'd like to do to reassure yourself). Simply running this command should work on MacOS, Linux, and Windows: uv run tool_install.py If it fails, it will try to tell you what prerequisite you are missing. Although it requires that you have at least installed uv and have some version of Python installed. To ensure that EPICpy correctly installed, try running it: EPICpy If you see the EPICpy graphical interface, it worked! To ensure that epiccoder correctly installed, try running it: epiccoder If you see the epiccoder graphical interface, it worked!","title":"Downloading EPICpy and epiccoder [EASY Approach]"},{"location":"installing/#downloading-epicpy-and-epiccoder-manual-approach","text":"To install EPICpy and epiccoder manually, first make sure you have successfully installed curl, git, uv, and either Python3.9 (if you run Windows), or Python3.10 (if you run Linux or MacOs). Next, you need to do the following: Obtain the path to python uv python list --only-installed Locate one of the lines containing \"3.9\" (Windows) or \"3.10\" (MacOS & Linux). For example, from this: cpython-3.12.7-linux-x86_64-gnu /home/nogard/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/bin/python3 -> python3.12 cpython-3.11.10-linux-x86_64-gnu /home/nogard/.local/share/uv/python/cpython-3.11.10-linux-x86_64-gnu/bin/python3 -> python3.11 cpython-3.10.12-linux-x86_64-gnu /usr/bin/python3.10 cpython-3.10.12-linux-x86_64-gnu /usr/bin/python3 -> python3.10 cpython-3.10.12-linux-x86_64-gnu /bin/python3.10 cpython-3.10.12-linux-x86_64-gnu /bin/python3 -> python3.10 cpython-3.9.20-linux-x86_64-gnu /home/nogard/.local/share/uv/python/cpython-3.9.20-linux-x86_64-gnu/bin/python3 -> python3.9 The structure of these entries is this: [NAME] [PATH] or [NAME] [PATH] -> [ALIAS] You want to copy the path. So if I choose cpython-3.9.20-linux-x86_64-gnu /home/nogard/.local/share/uv/python/cpython-3.9.20-linux-x86_64-gnu/bin/python3 -> python3.9 I would copy just /home/nogard/.local/share/uv/python/cpython-3.9.20-linux-x86_64-gnu/bin/python3 If I choose cpython-3.10.12-linux-x86_64-gnu /usr/bin/python3.10 then I would copy just /usr/bin/python3.10 Install EPICpy Because EPICpy has specific requirements for which Python version you use on each operating system, you have to include the appropriate python path in the installation command. Generically, the syntax would be like this: uv tool install git+https://www.github.com/travisseymour/EPICpy.git --python [YOUR PYTHON PATH] Specifically, you might enter something like this: # Linux example uv tool install git+https://www.github.com/travisseymour/EPICpy.git --python /usr/bin/python3.10 # MacOS example uv tool install git+https://www.github.com/travisseymour/EPICpy.git --python .local/share/uv/python/cpython-3.10.14-macos-x86_64-none/bin/python3 # Windows example uv tool install git+https://www.github.com/travisseymour/EPICpy.git --python AppData\\Local\\Programs\\Python39\\python.exe Of course, your python path may be different than these examples. Use the path you copied above as a result of running uv python list --only-installed . Install epiccoder epiccoder is less picky about which Python version you use. To install it, just run this: uv tool install git+https://www.github.com/travisseymour/epiccoder.git Testing Your Installations To ensure that EPICpy correctly installed, try running it: EPICpy If you see the EPICpy graphical interface, it worked! To ensure that epiccoder correctly installed, try running it: epiccoder If you see the epiccoder graphical interface, it worked!","title":"Downloading EPICpy and epiccoder [Manual Approach]"},{"location":"installing/#running-epic-simulations","text":"IMPORTANT : If you just installed EPICpy and epiccoder , then you have the latest version. However, if you installed EPICpy or epiccoder on your own machine during a previous session, you want to make sure you check for updates before running these tools. uv tool upgrade EPICpy epiccoder or use this script: uv run update_tools.py Now, run a simulation: To start EPICpy, just type EPICpy in a terminal. If this is your first time running EPICpy and the graphical interface seems wonky, then go to the menu and select Windows\ud83e\udc46RestoreDefaultLayout To load a model, you'll need to do: File\ud83e\udc46LoadDevice and then locate your device file. A device file is a python file like choice_device.py File\ud83e\udc46CompileRules and then locate your rules file. A rules file is a text file like choicetask_rules_VM.prs To run a model, you'll need to do: Run\ud83e\udc46Run Try to run the Choice task simulation with the \"VM\" rules 3 times in a row. If you do not see the colorful graph at the end, you may not have the Stats Window open. In the menu, select Windows\ud83e\udc46ShowStatsWindow .","title":"Running EPIC Simulations"},{"location":"model_programming/","text":"EPICpy Model Programming A detailed introduction to EPIC modeling is beyond this documentation, however, my book An EPIC Introduction to Computational Cognitive Modeling will be available soon! Below are just a few notes specific to these facilities in EPICpy specifically. For now, the best way to learn how devices, encoders, and rule-files work is to read the corresponding demo files carefully. Devices As discussed in the Introduction and the Simulation Components sections, a device is a virtual task with which the simulated human interacts. Devices typically (though not necessarily) derive from real-world tasks previously used with humans (or are candidate tasks that might later be used with humans). In any case, a device is a mini program that describes a task environment. The key difference between an EPIC device and a human task is that the device must specify the intersections between the virtual task and the virtual human. For example, rather than just placing a visual stimulus on the virtual display, the device must specify which perceptual information about visual stimuli will be transmitted to and potentially available to the virtual human. Critically, these properties may be a subset of the properties available to humans doing a related task. For example, this is the device code necessary to play a warning beep sound while presenting a '#' symbol on the display just north-east of the screen center: # Show fixation stimulus self.make_visual_object_appear(self.wstim_v_name, GU.Point(0., 0.), GU.Size(1., 1.)) self.set_visual_object_property(self.wstim_v_name, Text_c, Symbol(\"#\")) self.set_visual_object_property(self.wstim_v_name, Color_c, Black_c) # Play fixation beep wstim_snd_name = concatenate_to_Symbol(\"WarningSound\", self.trial) self.make_auditory_sound_event(wstim_snd_name, Symbol(\"Signal\"), GU.Point(0, -5), Symbol(\"Beep\"), 12, 300) These commands both cause a change in the task (draw something on the virtual display, play a sound via the virtual speakers), but they also signal the initiation of sensory onsets in the virtual performer. The device operation is event driven and is designed to fit within the discrete-event simulation framework EPIC employs. THe device starts by waiting for a START signal from the simulation controller and defines what behavior to take when this occurs, including what the next state is and what to do when this state is instantiated. Typically, other than START and STOP, most future states are triggered by time delays. That is, the device sets the current state to THE_NEXT_STATE and instructs the simulation to send a signal at some number of milliseconds in the future. At that point, the code for THE_NEXT_STATE is run and this all happens again. For example, this is a chunk of the Delay Event handler in the choice-task demo device: def handle_Delay_event(self): if self.state == state.START: self.state = state.START_TRIAL self.schedule_delay_event(500) elif self.state == state.START_TRIAL: self.vresponse_made = False self.start_trial() self.state = state.PRESENT_FIXATION self.schedule_delay_event(100) elif self.state == state.PRESENT_FIXATION: self.present_fixation_point() self.state = state.REMOVE_FIXATION self.schedule_delay_event(1000) elif self.state == state.REMOVE_FIXATION: self.remove_fixation_point() self.state = state.PRESENT_STIMULUS stimwaittime = random.randint(800, 1300) self.schedule_delay_event(stimwaittime) elif self.state == state.PRESENT_STIMULUS: self.present_stimulus() self.state = state.WAITING_FOR_RESPONSE self.schedule_delay_event(100) elif self.state == state.DISCARD_STIMULUS: # if there are more trials to run, remove_stimulus() sets # the state to START_TRIAL and schedules a delay self.remove_stimulus() elif self.state == state.SHUTDOWN: self.stop_simulation() Other signals may arrive to the device as well, e.g., if the simulated human produces any behavior (eye movements, hand movements, vocal movements, etc.), specific events are signaled that may have corresponding handlers in the device. These may do nothing, they may affect some internal data, they may alter the device's state, they may signal some future time event to occur, or they may do various combinations of these things. For example, the following is streamlined version of the choice-device's handler for a manual keyboard response: def handle_Keystroke_event(self, key_name: Symbol): # Compute RT and Accuracy rt = self.get_time() - self.vstim_onset if key_name == self.correct_vresp: outcome = \"CORRECT\" if self.trial > 1: self.current_vrt.update(rt) else: outcome = \"INCORRECT\" # Format and Save Next Row of Data if self.data_file: # note: order of values dictated by names in self.data_header data = (self.run_id, self.trial, self.task_name, self.task_difficulty, self.vstim_color, self.vstim_xloc, self.correct_vresp, key_name, 'Keyboard', rt, outcome, self.tag_str, self.device_name, self.rule_filename, datetime.now().ctime()) self.data_writer.writerow(data) self.vresponse_made = True # Change To Post-Reponse Event and Schedule 500 ms Delay self.state = state.DISCARD_STIMULUS self.schedule_delay_event(500) For a more detailed introduction, see the chapter Anatomy of A Device File ) in the upcoming book An EPIC Introduction to Computational Cognitive Modeling . Device Files in EPICpy Device files in EPICpy are python files that will import several components of the EPICLib library (via a Python C module compiled from the EPICLib C++ code). However, they do not import directly from EPICLib, instead, all device imports are through the epicpydevice package. For example, devices will minimally need access to the Device_base class and the Output_tee class. Consider these imports atop the choice_device.py file: from epicpydevice import epicpy_device_base from epicpydevice.output_tee import Output_tee from epicpydevice.epic_statistics import Mean_accumulator from epicpydevice.symbol_utilities import concatenate_to_Symbol, get_nth_Symbol from epicpydevice.device_exception import Device_exception from epicpydevice.speech_word import Speech_word import epicpydevice.geometric_utilities as GU from epicpydevice.symbol import Symbol from epicpydevice.standard_symbols import ( Red_c, Green_c, Blue_c, Yellow_c, Black_c, Text_c, Color_c, Shape_c, Circle_c, ) # etc Although you can name your device code file whatever you want, every device code file (e.g., see choice_device.py ) must contain a class named EpicDevice that derives from the class EpicPyDevice . The class EpicPyDevice is defined in a file called epicpy_device_base.py . to achieve this, every EPICpy device must import the epicpy_device_base module defined in the file epicpy_device_base.py. For example: from epicpydevice import epicpy_device_base class EpicDevice(epicpy_device_base.EpicPyDevice): def __init__(self, ot: Output_tee, parent: Any, device_folder: Path): epicpy_device_base.EpicPyDevice.__init__( self, parent=parent, ot=ot, device_name=\"Choice_Device_v2022.3\", device_folder=device_folder, ) # etc You may wonder why epicpy_device_base.py exists...why not have it be part of the EPICpy codebase or the EPIClib C++ codebase like all the other imports. The purpose of this separation is in service of those wanting to use a Python IDE for device and encoder programming without having to set up the entire EPICpy development environment. E.g., A desirable workflow would be to using the standalone EPICpy application to run EPIC simulations and use a Python IDE (e.g., PyCharm for writing/editing EPICpy devices and encoders. Such a workflow would thus not require the EPICpy codebase, any of its dependencies, or even Python itself, but would still offer code completion for any methods or properties defined within the EPICPyDevice class. Device Parameter Strings Device parameter strings are space delimited parameters used to specify a simulation run. For example, for the demo choice-device, the default parameter string is \"10 4 Easy, Draft\", corresponding to the number of trials, the number of colors to use, the difficulty condition, and a run-note to include. Traditionally, if a modeller wanted to run this simulation for both the Easy and Hard condition, it would be necessary to run \"10 4 Easy Draft\", then change it to \"10 4 Hard Draft\" and run the simulation again. EPICpy adds a new ability to specify parameter ranges in device parameter strings. For example, this parameter string: 10 4 [Easy|Hard] Draft when run would automatically create 2 parameter specifications, \"10 4 Easy Draft\" and \"10 4 Hard Draft\" that would run sequentially. The effort saved here is minimal (pushing RUN once vs twice and having to change the word Easy to the word Hard), but consider how many runs this parameter string represents and the effort saved: 80 4 [Easy|Hard] [r1|r12|r3|r4|r5|r6|r7|r8|r9|r10|] This one parameter string leads to 10 runs of 80-trials each of the Easy condition and the Hard condition. Thus, this one parameter string automates 20 separate simulation runs. Simulation User Options EPICpy allows device programmers to expose device options via the EPICpy GUI. For example, one common use is to let the modeller decide whether or not certain debug messages are shown during a simulation. What kinds of options are available are entirely up to the device programmer, who only needs to specify a dictionary called self.option filled with string:boolean key:value pairs. For example, here are the options defined in the demo choice device: # Optionally expose some boolean device options to the user via the GUI. self.option = dict() # from EpicPyDevice, default is dict() # useful for showing debug information during development self.option[\"show_debug_messages\"] = False # useful for showing device state info during trial run self.option[\"show_trial_events\"] = False # useful for outputting trial parameters and data during task run self.option[\"show_trial_data\"] = True # useful for long description of the task and parameters self.option[\"show_task_description\"] = True Corresponding code in the device would need conditions that take advantage of these options, e.g.: if self.option['show_debug_messages']: self.write('Trial Starting...') They show up in the EPICapp GUI as a series of toggles in the Device Options dialog: Simulation Data Stimulation data is entirely up to the device programmer who decides what is stored and at what interval. EPICpy does assume that a file called data_output.csv will be produced following each simulation run, but will not freak out if this is not the case. A device would typically create a CSV (comma separated value) text file where each row is a trial and each column is some datum referring to either the trial or event parameters, or some aspect of EPIC's simulated behavior. However, this is not required and anything is possible. It would even be fine to store simulation data across multiple files, or even read in simulation parameters from a file or files in already on disk. Simulation Graphs and Analysis EPIC devices often output simple stats following a run, e.g., the mean RT or accuracy of a simulation run. In some devices, the statistical computations are more involved and may include computing lane deviation, root mean squared error (RMSE), and other statistics. A facility to do these types of computations already exist in EPIClib in the statistics module (Statistics.h). In some cases, it may be helpful to know following a run if the task RTs differ for correct and incorrect responses (e.g., a t-test), or whether RTs differ as a function of stimulus color (e.g., an ANOVA). Such functions do not already exist in EPIC and the modeler would have to either create such code herself (not a great idea), or open the output data in a statistical program and run the analysis there. Another common part of a modeler's workflow not currently supported by EPIClib is data graphing. Not only can it be helpful to view graphs of simulated data, it is particularly helpful to view graphs that compare simulated data to human data or idealized data. This step also requires the EPIC modeler to move to another program. EPICpy allows for both simple and complex data analysis, as well as simple and complex graphing to be specified right in the device file and output directly within the EPICpy GUI, eliminating the need to add statistical and graphing applications to the modeling workflow. For statistical needs, EPICpy offers all the facilities of the Pingouin package https://pingouin-stats.org/ , which includes the ability for EPICpy device modellers to import and use all of its dependencies. Available packages include NumPy , SciPy , Pandas , and Statsmodels . For graphing, Pingouin provides access to both Matplotlib and Seaborn packages. For example, the following is a subset of the analysis code evaluated at the end of each demo choice-task run: # NOTE: Some Steps Have Been Omitted, see choice_device.py for full code # display means table table = data.groupby(['StimColor'])['RT'].agg(['mean', 'count']) table = table.astype(int) self.stats_write(table.transpose()) # display ANOVA aov = pg.rm_anova(data, dv='RT', subject='RunID', within='StimColor') self.stats_write(aov) # display graph fig, ax = plt.subplots(figsize=(7, 4), dpi=96) my_plot = sns.barplot(x=\"StimColor\", y=\"RT\", data=data, capsize=.1, ax=ax) plt.title(f'Mean RT by Stimulus Color, {cond} Condition') plt.xlabel('Stimulus Color') plt.ylabel('Mean Response Time (ms)') self.stats_write(my_plot.get_figure()) Note that although devices use self.write() to write text to the Normal Output Window , they use self.stats_write() to write text, tables, and graphs to the Stats Output Window . For example (the code above is for a One-way ANOVA, but the image below is actually for a Two-way ANOVA): Using Device Underlay Images EPICpy allows modelers to show pictures underneath the normal display contents. The purpose of this facility is entirely for demonstration purposes. A likely case is when one wants to show a simulation with abstracted task stimuli overtop of the original (presumably more feature rich) task environment. For example, if you wanted to understand human driving behavior in a simulator like this: but you were using an EPICpy device that produces displays like this: you might want to use an overlay when giving a talk or other situation where visuals were more important than simulation run speed (it's faster to run without display images): This under-display image must be enabled in the device via code such as this: The image must be in a folder called images next to the device file. Finally, to actually see device underlay images, the setting \" Allow device to draw view underlay images \" must be enabled in the GUI's Display Controls panel: Perceptual Encoders As discussed in the Simulation Components section, a perceptual encoder allows EPIC modelers to alter the way visual and auditory stimuli are represented in EPIC's perceptual working memories. For example, imagine one is modeling a car dashboard task to understand how drivers respond to various warning lights. For example: Now it is possible to model how humans visually identify and locate the warnings, but this would require that the device specify all the details needed to perceptually segment and identify each warning light. That's fine, provided one had a sufficient psychological model to employ and a set of production rules to embody a likely looking strategy and a way to move from shape identification to conceptual identification. However, if you wanted to start your simulation with a virtual performer that already knew which shapes and color pairs indicated the battery indicator, the tire warning, etc, this would be overkill. Instead, you might create a device that used the following shapes/colors to indicate the various warnings: To represent a human driver that already knows that the red rectangle thingy in the bottom left is the battery operator, you could do one of the following write a production rule that fires when a red rectangle is perceived in EPIC's Visual Working Memory (WM) and creates an item in EPIC's Amodal WM tagging that object as the battery indicator. E.g.: (TRIAL_identify_battery_indicator If ( (Goal Do Driving_Task) (Step Scan Dashboard) (Visual ?object Shape Rectangle) (Visual ?object Color Red) (NOT (Tag ??? BatteryInddicator)) ) Then ( (Add (Tag ?object BatteryInddicator)) ) ) Another option is to create a visual encoder that allows EPIC to bypass this step and directly perceive the Shape property value Rectangle as the \"shape\" BatteryIndicator, and the Shape property Plus as the \"shape\" EcoModeIndicator. In this case, the role of the visual encoder is embody a type of perceptual expertise whereby particpants have sufficient experience with the interface that they no longer must translate the rectangle into the concept of BatteryIndicator. Perceptual Encoder Files in EPICpy Like device files, perceptual encoders are python code files. Because they are loaded by the user via the GUI, it doesn't matter what you name your encoder files, just that they be text files containing Python code and have a .py file extension. Another requirements for encoders is that they import the proper encoder base from EPIClib. For example, for a visual encoder: from epicpydevice.epicpy_visual_encoder_base import EPICPyVisualEncoder Next, you must either define a class named VisualEncoder that derives from Visual_encoder_base or a class named AuditoryEncoder that derives from Auditory_encoder_base . For example, for a visual encoder: class VisualEncoder(EPICPyVisualEncoder): def __init__(self, encoder_name: str, parent: Any = None): super(VisualEncoder, self).__init__( encoder_name=encoder_name if encoder_name else \"VisualEncoder\", parent=parent, ) # etc. To achieve the encoder functionality described above, we could have the device use a distinct simple shape for each car warning icon (circle, rectangle, triangle, etc.) and then write an encoder that re-encoded those simple shapes into the coneptual shapes drivers have learned, e.g.: def set_object_property(self, object_name: Symbol, property_name: Symbol, property_value: Symbol, encoding_time: int): if property_name == Shape_c: # re-encode simple shapes as driver-learned warning shapes if property_value == Filled_Rectangle_c: encoded_value = Symbol('BatteryWarning') elif property_value == Filled_Circle_c: encoded_value = Symbol('SteeringWarning') elif property_value == Filled_Triangle_c: encoded_value = Symbol('KeyFobWarning') ... elif: # other shapes are not being re-encoded encoded_value = property_value # transmit encoded shape property value if in above set, # otherwise, pass along the original value self.schedule_change_property_event(encoding_time, object_name, property_name, encoded_value) else: # not a shape property, this encoding doesn't apply return False Note that this approach is only advisable if the behavior being modeled is not expected to be significantly affected by the time it takes to perceive the warning shapes. Otherwise, your model may not be particularly compelling. Another use of encoders may be to represent encoding error. E.g., rather than creating a set of production rules that embody a full model of why and how people mis-perceive stimulus properties, a modeler may decide to simply codify the probability of making an encoding error and hard-code the expected error. For example, the visual encoder distributed with the demo choice-device randomly mis-encodes some stimulus colors at a specified failure rate: def set_object_property(self, object_name: Symbol, property_name: Symbol, property_value: Symbol, encoding_time: int) -> bool: \"\"\" Imparts a self.recoding_failure_rate chance of mis-perceiving object colors: Yellow->Blue & Green->Red. If not overridden, this method just returns False to indicate that the encoding is not being handled here. \"\"\" # this encoding does not apply if property_name != Color_c: return False # failure rate is 0, nothing to do if not self.recoding_failure_rate: return False # can only confuse green and yellow if property_value not in (Yellow_c, Green_c, Nil_c): return False # flip a coin to decide whether encoding is successful successful_encoding = ( unit_uniform_random_variable() > self.recoding_failure_rate ) if property_value == Nil_c: # previous property values need to be removed! encoding = Nil_c elif property_value == Yellow_c: # random chance of perceiving yellow as blue! encoding = property_value if successful_encoding else Blue_c elif property_value == Green_c: # random chance of perceiving yellow as red! encoding = property_value if successful_encoding else Red_c else: return False # this encoding does not apply # transmit forward the encoded Shape self.schedule_change_property_event( encoding_time, object_name, property_name, encoding ) Production Rules Production rules are briefly described in the Task Rules section of the Simulation Components page. EPICpy uses the exact same production rule syntax as the original C++ version of EPIC. Reading over the rules provided with the demo devices is a good way to learn how rules work, but this should be augmented with a careful read of EPIC Principles of Operation (2004) by David Kieras. Although unnecessary for most modelers, to dive deeper and understand how EPIC's production-rule parser works, you can read PPS: A Parsimonious Production System (1987) by Arie Covrigaru and David Kieras. There will soon be a textbook available that describes EPIC production rule programming in detail for beginners.","title":"Model Programming"},{"location":"model_programming/#epicpy-model-programming","text":"A detailed introduction to EPIC modeling is beyond this documentation, however, my book An EPIC Introduction to Computational Cognitive Modeling will be available soon! Below are just a few notes specific to these facilities in EPICpy specifically. For now, the best way to learn how devices, encoders, and rule-files work is to read the corresponding demo files carefully.","title":"EPICpy Model Programming"},{"location":"model_programming/#devices","text":"As discussed in the Introduction and the Simulation Components sections, a device is a virtual task with which the simulated human interacts. Devices typically (though not necessarily) derive from real-world tasks previously used with humans (or are candidate tasks that might later be used with humans). In any case, a device is a mini program that describes a task environment. The key difference between an EPIC device and a human task is that the device must specify the intersections between the virtual task and the virtual human. For example, rather than just placing a visual stimulus on the virtual display, the device must specify which perceptual information about visual stimuli will be transmitted to and potentially available to the virtual human. Critically, these properties may be a subset of the properties available to humans doing a related task. For example, this is the device code necessary to play a warning beep sound while presenting a '#' symbol on the display just north-east of the screen center: # Show fixation stimulus self.make_visual_object_appear(self.wstim_v_name, GU.Point(0., 0.), GU.Size(1., 1.)) self.set_visual_object_property(self.wstim_v_name, Text_c, Symbol(\"#\")) self.set_visual_object_property(self.wstim_v_name, Color_c, Black_c) # Play fixation beep wstim_snd_name = concatenate_to_Symbol(\"WarningSound\", self.trial) self.make_auditory_sound_event(wstim_snd_name, Symbol(\"Signal\"), GU.Point(0, -5), Symbol(\"Beep\"), 12, 300) These commands both cause a change in the task (draw something on the virtual display, play a sound via the virtual speakers), but they also signal the initiation of sensory onsets in the virtual performer. The device operation is event driven and is designed to fit within the discrete-event simulation framework EPIC employs. THe device starts by waiting for a START signal from the simulation controller and defines what behavior to take when this occurs, including what the next state is and what to do when this state is instantiated. Typically, other than START and STOP, most future states are triggered by time delays. That is, the device sets the current state to THE_NEXT_STATE and instructs the simulation to send a signal at some number of milliseconds in the future. At that point, the code for THE_NEXT_STATE is run and this all happens again. For example, this is a chunk of the Delay Event handler in the choice-task demo device: def handle_Delay_event(self): if self.state == state.START: self.state = state.START_TRIAL self.schedule_delay_event(500) elif self.state == state.START_TRIAL: self.vresponse_made = False self.start_trial() self.state = state.PRESENT_FIXATION self.schedule_delay_event(100) elif self.state == state.PRESENT_FIXATION: self.present_fixation_point() self.state = state.REMOVE_FIXATION self.schedule_delay_event(1000) elif self.state == state.REMOVE_FIXATION: self.remove_fixation_point() self.state = state.PRESENT_STIMULUS stimwaittime = random.randint(800, 1300) self.schedule_delay_event(stimwaittime) elif self.state == state.PRESENT_STIMULUS: self.present_stimulus() self.state = state.WAITING_FOR_RESPONSE self.schedule_delay_event(100) elif self.state == state.DISCARD_STIMULUS: # if there are more trials to run, remove_stimulus() sets # the state to START_TRIAL and schedules a delay self.remove_stimulus() elif self.state == state.SHUTDOWN: self.stop_simulation() Other signals may arrive to the device as well, e.g., if the simulated human produces any behavior (eye movements, hand movements, vocal movements, etc.), specific events are signaled that may have corresponding handlers in the device. These may do nothing, they may affect some internal data, they may alter the device's state, they may signal some future time event to occur, or they may do various combinations of these things. For example, the following is streamlined version of the choice-device's handler for a manual keyboard response: def handle_Keystroke_event(self, key_name: Symbol): # Compute RT and Accuracy rt = self.get_time() - self.vstim_onset if key_name == self.correct_vresp: outcome = \"CORRECT\" if self.trial > 1: self.current_vrt.update(rt) else: outcome = \"INCORRECT\" # Format and Save Next Row of Data if self.data_file: # note: order of values dictated by names in self.data_header data = (self.run_id, self.trial, self.task_name, self.task_difficulty, self.vstim_color, self.vstim_xloc, self.correct_vresp, key_name, 'Keyboard', rt, outcome, self.tag_str, self.device_name, self.rule_filename, datetime.now().ctime()) self.data_writer.writerow(data) self.vresponse_made = True # Change To Post-Reponse Event and Schedule 500 ms Delay self.state = state.DISCARD_STIMULUS self.schedule_delay_event(500) For a more detailed introduction, see the chapter Anatomy of A Device File ) in the upcoming book An EPIC Introduction to Computational Cognitive Modeling .","title":"Devices"},{"location":"model_programming/#device-files-in-epicpy","text":"Device files in EPICpy are python files that will import several components of the EPICLib library (via a Python C module compiled from the EPICLib C++ code). However, they do not import directly from EPICLib, instead, all device imports are through the epicpydevice package. For example, devices will minimally need access to the Device_base class and the Output_tee class. Consider these imports atop the choice_device.py file: from epicpydevice import epicpy_device_base from epicpydevice.output_tee import Output_tee from epicpydevice.epic_statistics import Mean_accumulator from epicpydevice.symbol_utilities import concatenate_to_Symbol, get_nth_Symbol from epicpydevice.device_exception import Device_exception from epicpydevice.speech_word import Speech_word import epicpydevice.geometric_utilities as GU from epicpydevice.symbol import Symbol from epicpydevice.standard_symbols import ( Red_c, Green_c, Blue_c, Yellow_c, Black_c, Text_c, Color_c, Shape_c, Circle_c, ) # etc Although you can name your device code file whatever you want, every device code file (e.g., see choice_device.py ) must contain a class named EpicDevice that derives from the class EpicPyDevice . The class EpicPyDevice is defined in a file called epicpy_device_base.py . to achieve this, every EPICpy device must import the epicpy_device_base module defined in the file epicpy_device_base.py. For example: from epicpydevice import epicpy_device_base class EpicDevice(epicpy_device_base.EpicPyDevice): def __init__(self, ot: Output_tee, parent: Any, device_folder: Path): epicpy_device_base.EpicPyDevice.__init__( self, parent=parent, ot=ot, device_name=\"Choice_Device_v2022.3\", device_folder=device_folder, ) # etc You may wonder why epicpy_device_base.py exists...why not have it be part of the EPICpy codebase or the EPIClib C++ codebase like all the other imports. The purpose of this separation is in service of those wanting to use a Python IDE for device and encoder programming without having to set up the entire EPICpy development environment. E.g., A desirable workflow would be to using the standalone EPICpy application to run EPIC simulations and use a Python IDE (e.g., PyCharm for writing/editing EPICpy devices and encoders. Such a workflow would thus not require the EPICpy codebase, any of its dependencies, or even Python itself, but would still offer code completion for any methods or properties defined within the EPICPyDevice class. Device Parameter Strings Device parameter strings are space delimited parameters used to specify a simulation run. For example, for the demo choice-device, the default parameter string is \"10 4 Easy, Draft\", corresponding to the number of trials, the number of colors to use, the difficulty condition, and a run-note to include. Traditionally, if a modeller wanted to run this simulation for both the Easy and Hard condition, it would be necessary to run \"10 4 Easy Draft\", then change it to \"10 4 Hard Draft\" and run the simulation again. EPICpy adds a new ability to specify parameter ranges in device parameter strings. For example, this parameter string: 10 4 [Easy|Hard] Draft when run would automatically create 2 parameter specifications, \"10 4 Easy Draft\" and \"10 4 Hard Draft\" that would run sequentially. The effort saved here is minimal (pushing RUN once vs twice and having to change the word Easy to the word Hard), but consider how many runs this parameter string represents and the effort saved: 80 4 [Easy|Hard] [r1|r12|r3|r4|r5|r6|r7|r8|r9|r10|] This one parameter string leads to 10 runs of 80-trials each of the Easy condition and the Hard condition. Thus, this one parameter string automates 20 separate simulation runs. Simulation User Options EPICpy allows device programmers to expose device options via the EPICpy GUI. For example, one common use is to let the modeller decide whether or not certain debug messages are shown during a simulation. What kinds of options are available are entirely up to the device programmer, who only needs to specify a dictionary called self.option filled with string:boolean key:value pairs. For example, here are the options defined in the demo choice device: # Optionally expose some boolean device options to the user via the GUI. self.option = dict() # from EpicPyDevice, default is dict() # useful for showing debug information during development self.option[\"show_debug_messages\"] = False # useful for showing device state info during trial run self.option[\"show_trial_events\"] = False # useful for outputting trial parameters and data during task run self.option[\"show_trial_data\"] = True # useful for long description of the task and parameters self.option[\"show_task_description\"] = True Corresponding code in the device would need conditions that take advantage of these options, e.g.: if self.option['show_debug_messages']: self.write('Trial Starting...') They show up in the EPICapp GUI as a series of toggles in the Device Options dialog: Simulation Data Stimulation data is entirely up to the device programmer who decides what is stored and at what interval. EPICpy does assume that a file called data_output.csv will be produced following each simulation run, but will not freak out if this is not the case. A device would typically create a CSV (comma separated value) text file where each row is a trial and each column is some datum referring to either the trial or event parameters, or some aspect of EPIC's simulated behavior. However, this is not required and anything is possible. It would even be fine to store simulation data across multiple files, or even read in simulation parameters from a file or files in already on disk. Simulation Graphs and Analysis EPIC devices often output simple stats following a run, e.g., the mean RT or accuracy of a simulation run. In some devices, the statistical computations are more involved and may include computing lane deviation, root mean squared error (RMSE), and other statistics. A facility to do these types of computations already exist in EPIClib in the statistics module (Statistics.h). In some cases, it may be helpful to know following a run if the task RTs differ for correct and incorrect responses (e.g., a t-test), or whether RTs differ as a function of stimulus color (e.g., an ANOVA). Such functions do not already exist in EPIC and the modeler would have to either create such code herself (not a great idea), or open the output data in a statistical program and run the analysis there. Another common part of a modeler's workflow not currently supported by EPIClib is data graphing. Not only can it be helpful to view graphs of simulated data, it is particularly helpful to view graphs that compare simulated data to human data or idealized data. This step also requires the EPIC modeler to move to another program. EPICpy allows for both simple and complex data analysis, as well as simple and complex graphing to be specified right in the device file and output directly within the EPICpy GUI, eliminating the need to add statistical and graphing applications to the modeling workflow. For statistical needs, EPICpy offers all the facilities of the Pingouin package https://pingouin-stats.org/ , which includes the ability for EPICpy device modellers to import and use all of its dependencies. Available packages include NumPy , SciPy , Pandas , and Statsmodels . For graphing, Pingouin provides access to both Matplotlib and Seaborn packages. For example, the following is a subset of the analysis code evaluated at the end of each demo choice-task run: # NOTE: Some Steps Have Been Omitted, see choice_device.py for full code # display means table table = data.groupby(['StimColor'])['RT'].agg(['mean', 'count']) table = table.astype(int) self.stats_write(table.transpose()) # display ANOVA aov = pg.rm_anova(data, dv='RT', subject='RunID', within='StimColor') self.stats_write(aov) # display graph fig, ax = plt.subplots(figsize=(7, 4), dpi=96) my_plot = sns.barplot(x=\"StimColor\", y=\"RT\", data=data, capsize=.1, ax=ax) plt.title(f'Mean RT by Stimulus Color, {cond} Condition') plt.xlabel('Stimulus Color') plt.ylabel('Mean Response Time (ms)') self.stats_write(my_plot.get_figure()) Note that although devices use self.write() to write text to the Normal Output Window , they use self.stats_write() to write text, tables, and graphs to the Stats Output Window . For example (the code above is for a One-way ANOVA, but the image below is actually for a Two-way ANOVA): Using Device Underlay Images EPICpy allows modelers to show pictures underneath the normal display contents. The purpose of this facility is entirely for demonstration purposes. A likely case is when one wants to show a simulation with abstracted task stimuli overtop of the original (presumably more feature rich) task environment. For example, if you wanted to understand human driving behavior in a simulator like this: but you were using an EPICpy device that produces displays like this: you might want to use an overlay when giving a talk or other situation where visuals were more important than simulation run speed (it's faster to run without display images): This under-display image must be enabled in the device via code such as this: The image must be in a folder called images next to the device file. Finally, to actually see device underlay images, the setting \" Allow device to draw view underlay images \" must be enabled in the GUI's Display Controls panel:","title":"Device Files in EPICpy"},{"location":"model_programming/#perceptual-encoders","text":"As discussed in the Simulation Components section, a perceptual encoder allows EPIC modelers to alter the way visual and auditory stimuli are represented in EPIC's perceptual working memories. For example, imagine one is modeling a car dashboard task to understand how drivers respond to various warning lights. For example: Now it is possible to model how humans visually identify and locate the warnings, but this would require that the device specify all the details needed to perceptually segment and identify each warning light. That's fine, provided one had a sufficient psychological model to employ and a set of production rules to embody a likely looking strategy and a way to move from shape identification to conceptual identification. However, if you wanted to start your simulation with a virtual performer that already knew which shapes and color pairs indicated the battery indicator, the tire warning, etc, this would be overkill. Instead, you might create a device that used the following shapes/colors to indicate the various warnings: To represent a human driver that already knows that the red rectangle thingy in the bottom left is the battery operator, you could do one of the following write a production rule that fires when a red rectangle is perceived in EPIC's Visual Working Memory (WM) and creates an item in EPIC's Amodal WM tagging that object as the battery indicator. E.g.: (TRIAL_identify_battery_indicator If ( (Goal Do Driving_Task) (Step Scan Dashboard) (Visual ?object Shape Rectangle) (Visual ?object Color Red) (NOT (Tag ??? BatteryInddicator)) ) Then ( (Add (Tag ?object BatteryInddicator)) ) ) Another option is to create a visual encoder that allows EPIC to bypass this step and directly perceive the Shape property value Rectangle as the \"shape\" BatteryIndicator, and the Shape property Plus as the \"shape\" EcoModeIndicator. In this case, the role of the visual encoder is embody a type of perceptual expertise whereby particpants have sufficient experience with the interface that they no longer must translate the rectangle into the concept of BatteryIndicator.","title":"Perceptual Encoders"},{"location":"model_programming/#perceptual-encoder-files-in-epicpy","text":"Like device files, perceptual encoders are python code files. Because they are loaded by the user via the GUI, it doesn't matter what you name your encoder files, just that they be text files containing Python code and have a .py file extension. Another requirements for encoders is that they import the proper encoder base from EPIClib. For example, for a visual encoder: from epicpydevice.epicpy_visual_encoder_base import EPICPyVisualEncoder Next, you must either define a class named VisualEncoder that derives from Visual_encoder_base or a class named AuditoryEncoder that derives from Auditory_encoder_base . For example, for a visual encoder: class VisualEncoder(EPICPyVisualEncoder): def __init__(self, encoder_name: str, parent: Any = None): super(VisualEncoder, self).__init__( encoder_name=encoder_name if encoder_name else \"VisualEncoder\", parent=parent, ) # etc. To achieve the encoder functionality described above, we could have the device use a distinct simple shape for each car warning icon (circle, rectangle, triangle, etc.) and then write an encoder that re-encoded those simple shapes into the coneptual shapes drivers have learned, e.g.: def set_object_property(self, object_name: Symbol, property_name: Symbol, property_value: Symbol, encoding_time: int): if property_name == Shape_c: # re-encode simple shapes as driver-learned warning shapes if property_value == Filled_Rectangle_c: encoded_value = Symbol('BatteryWarning') elif property_value == Filled_Circle_c: encoded_value = Symbol('SteeringWarning') elif property_value == Filled_Triangle_c: encoded_value = Symbol('KeyFobWarning') ... elif: # other shapes are not being re-encoded encoded_value = property_value # transmit encoded shape property value if in above set, # otherwise, pass along the original value self.schedule_change_property_event(encoding_time, object_name, property_name, encoded_value) else: # not a shape property, this encoding doesn't apply return False Note that this approach is only advisable if the behavior being modeled is not expected to be significantly affected by the time it takes to perceive the warning shapes. Otherwise, your model may not be particularly compelling. Another use of encoders may be to represent encoding error. E.g., rather than creating a set of production rules that embody a full model of why and how people mis-perceive stimulus properties, a modeler may decide to simply codify the probability of making an encoding error and hard-code the expected error. For example, the visual encoder distributed with the demo choice-device randomly mis-encodes some stimulus colors at a specified failure rate: def set_object_property(self, object_name: Symbol, property_name: Symbol, property_value: Symbol, encoding_time: int) -> bool: \"\"\" Imparts a self.recoding_failure_rate chance of mis-perceiving object colors: Yellow->Blue & Green->Red. If not overridden, this method just returns False to indicate that the encoding is not being handled here. \"\"\" # this encoding does not apply if property_name != Color_c: return False # failure rate is 0, nothing to do if not self.recoding_failure_rate: return False # can only confuse green and yellow if property_value not in (Yellow_c, Green_c, Nil_c): return False # flip a coin to decide whether encoding is successful successful_encoding = ( unit_uniform_random_variable() > self.recoding_failure_rate ) if property_value == Nil_c: # previous property values need to be removed! encoding = Nil_c elif property_value == Yellow_c: # random chance of perceiving yellow as blue! encoding = property_value if successful_encoding else Blue_c elif property_value == Green_c: # random chance of perceiving yellow as red! encoding = property_value if successful_encoding else Red_c else: return False # this encoding does not apply # transmit forward the encoded Shape self.schedule_change_property_event( encoding_time, object_name, property_name, encoding )","title":"Perceptual Encoder Files in EPICpy"},{"location":"model_programming/#production-rules","text":"Production rules are briefly described in the Task Rules section of the Simulation Components page. EPICpy uses the exact same production rule syntax as the original C++ version of EPIC. Reading over the rules provided with the demo devices is a good way to learn how rules work, but this should be augmented with a careful read of EPIC Principles of Operation (2004) by David Kieras. Although unnecessary for most modelers, to dive deeper and understand how EPIC's production-rule parser works, you can read PPS: A Parsimonious Production System (1987) by Arie Covrigaru and David Kieras. There will soon be a textbook available that describes EPIC production rule programming in detail for beginners.","title":"Production Rules"},{"location":"text_editor_for_epic_modeling/","text":"Text Editors for EPIC Modeling. Although almost any real text editor will work for EPIC modeling, I recommend you use a programmer's text editor . Often, the default \"text editor\" on an operating system is ill-suited for modeling. In particular, TextEdit.app on MacOS and Notepad.exe on Windows are poor choices. I personally use Sublime Text for editing EPIC rules. In my classes on EPIC, I have had good luck with the Pulsar Editor (based on the now abandoned Atom Editor ). Finally, in the before time (in the long-long ago), when I used Apple computers, I used BBEdit (formerly TextWrangler). There are dozens of good programmers text editors. The reason I'm mentioning the ones I use or have used in the past is because I have created syntax-highlighting definitions for EPIC production rules when using those editors. In case you don't know what syntax-highlighting is. Be default, if you were to load a rule file into one of these editors (or any other editor), it would look like this: However, with syntax highlighting enabled, the same text would appear like this: This is much easier to read and results in fewer errors of various kinds. Below are instructions for installing text editors for which I have already created syntax-highlighting definitions for. They are cross-platform and free to use unless otherwise noted. Pulsar Text Editor (Formerly Atom) Installing Pulsar You can download an executable installer for Pulsar on your operating system here: https://pulsar-edit.dev/download.html Set Up Pulsar to Syntax-Highlight EPIC Rule Files Linux and MacOS Close Pulsar and then enter this into a terminal: ppm install language-epic Windows On Windows, installing Pulsar will not put ppm on your path. In fact, it puts the old apm file in a folder on your machine something like this: C:\\Users\\YOURUSERNAME\\AppData\\Local\\Programs\\Pulsar\\resources\\app\\ppm\\bin Therefore, to install the syntax highlighting file on Windows, you will enter something like this: C:\\Users\\YOURUSERNAME\\AppData\\Local\\Programs\\Pulsar\\resources\\app\\ppm\\bin\\apm.cmd install language-epic Try it! Now open Pulsar and load a .prs rule file. You should see pretty colors. NOTE : If you have previous experience with Atom or Pulsar, you may wonder why I'm not suggesting that this package be installed from the GUI facility within Pulsar. Currently, only about 80 packages appear available this way and language-epic isn't one of them, even though it was formerly available in both Atom and Pulsar. Sublime Text Editor [ PAID, but continuous free trial ] Download Sublime Text You can obtain Sublime Text at https://www.sublimetext.com/ . These instructions should work with the current version of Sublime Text (currently, version 4.x), but I've only tested them with Sublime Text version 3.x . Install Package Control Open the Tools menu. Select Package Control This will download the latest version of Package Control. If that doesn't work , use this manual method (copied from https://packagecontrol.io/installation): Click the Preferences > Browse Packages\u2026 menu Browse up a folder and then into the Installed Packages/ folder Download Package Control.sublime-package and copy it into the Installed Packages/ directory Restart Sublime Text Install Package Development Tool Go to Preferences --> Package Control . Then type in install until you see \" Package Control: Install Package \" and click on it. Now type PackageDev and click on it. Wait a moment (a progress messages may briefly show in the bottom left of Sublime Text's main window.) Install the Syntax Highlighting Definition for EPIC's Rule Files Go to Tools --> Packages --> Package Development --> New Syntax Definition Sublime will give you a sample syntax definition file called \" untitled \". Clear out all the default text. Copy and paste the contents of the Subline Text EPIC-rules Syntax Highlighting File into this window. Now go to the menu and choose File --> Save As . Note that it will automatically point to some folder like Sublime-text/Packages/User , which is the right location! Save the file you just edited into this folder with this exact filename: \" epicrules.sublime-syntax \". Try it Open up any .prs file, it should automatically be detected as a rule file and you should see colorful syntax highlighting! BBEdit Text Editor [ MacOS Only ] Downloading BBEdit There is a paid and free mode of BBEdit. Here is what they say about this on their webpage: BBEdit offers a 30-day evaluation period. During that period, all of BBEdit\u2019s features are available. When it ends, you can still use BBEdit \u2014 with no nag screens or unsolicited interruptions, for free, forever. After the evaluation period, you may re-enable all of BBEdit\u2019s exclusive features at any time by purchasing a license. Download BBEdit from here: http://www.barebones.com/products/bbedit/download.html . Install like any other Mac app. Settling Up Syntax Highlighting for EPIC Rule Files Download the BBEdit EPIC-rules Syntax Highlighting File and save it as productionrule.plist . You may need to start and then close BBEdit for the following instructions to work: copy the file productionrule.plist to the folder /Users/YOURUSERNAME/Library/Application Support/BBEdit/Language Modules/ NOTE : If somehow you are using an old copy of TextWrangler instead of BBEdit (which has superseded it), then this will still work if you copy productionrule.plist to this folder instead: /Users/YOURUSERNAME/Library/Application Support/TextWrangler/Language Modules/ Try it Open up any .prs file, it should automatically be detected as a production-rule file, and you should see colorful syntax highlighting!","title":"Text Editors for EPCIpy"},{"location":"text_editor_for_epic_modeling/#text-editors-for-epic-modeling","text":"Although almost any real text editor will work for EPIC modeling, I recommend you use a programmer's text editor . Often, the default \"text editor\" on an operating system is ill-suited for modeling. In particular, TextEdit.app on MacOS and Notepad.exe on Windows are poor choices. I personally use Sublime Text for editing EPIC rules. In my classes on EPIC, I have had good luck with the Pulsar Editor (based on the now abandoned Atom Editor ). Finally, in the before time (in the long-long ago), when I used Apple computers, I used BBEdit (formerly TextWrangler). There are dozens of good programmers text editors. The reason I'm mentioning the ones I use or have used in the past is because I have created syntax-highlighting definitions for EPIC production rules when using those editors. In case you don't know what syntax-highlighting is. Be default, if you were to load a rule file into one of these editors (or any other editor), it would look like this: However, with syntax highlighting enabled, the same text would appear like this: This is much easier to read and results in fewer errors of various kinds. Below are instructions for installing text editors for which I have already created syntax-highlighting definitions for. They are cross-platform and free to use unless otherwise noted.","title":"Text Editors for EPIC Modeling."},{"location":"text_editor_for_epic_modeling/#pulsar-text-editor-formerly-atom","text":"","title":"Pulsar Text Editor (Formerly Atom)"},{"location":"text_editor_for_epic_modeling/#installing-pulsar","text":"You can download an executable installer for Pulsar on your operating system here: https://pulsar-edit.dev/download.html","title":"Installing Pulsar"},{"location":"text_editor_for_epic_modeling/#set-up-pulsar-to-syntax-highlight-epic-rule-files","text":"","title":"Set Up Pulsar to Syntax-Highlight EPIC Rule Files"},{"location":"text_editor_for_epic_modeling/#linux-and-macos","text":"Close Pulsar and then enter this into a terminal: ppm install language-epic","title":"Linux and MacOS"},{"location":"text_editor_for_epic_modeling/#windows","text":"On Windows, installing Pulsar will not put ppm on your path. In fact, it puts the old apm file in a folder on your machine something like this: C:\\Users\\YOURUSERNAME\\AppData\\Local\\Programs\\Pulsar\\resources\\app\\ppm\\bin Therefore, to install the syntax highlighting file on Windows, you will enter something like this: C:\\Users\\YOURUSERNAME\\AppData\\Local\\Programs\\Pulsar\\resources\\app\\ppm\\bin\\apm.cmd install language-epic","title":"Windows"},{"location":"text_editor_for_epic_modeling/#try-it","text":"Now open Pulsar and load a .prs rule file. You should see pretty colors. NOTE : If you have previous experience with Atom or Pulsar, you may wonder why I'm not suggesting that this package be installed from the GUI facility within Pulsar. Currently, only about 80 packages appear available this way and language-epic isn't one of them, even though it was formerly available in both Atom and Pulsar.","title":"Try it!"},{"location":"text_editor_for_epic_modeling/#sublime-text-editor-paid-but-continuous-free-trial","text":"","title":"Sublime Text Editor [PAID, but continuous free trial]"},{"location":"text_editor_for_epic_modeling/#download-sublime-text","text":"You can obtain Sublime Text at https://www.sublimetext.com/ . These instructions should work with the current version of Sublime Text (currently, version 4.x), but I've only tested them with Sublime Text version 3.x .","title":"Download Sublime Text"},{"location":"text_editor_for_epic_modeling/#install-package-control","text":"Open the Tools menu. Select Package Control This will download the latest version of Package Control. If that doesn't work , use this manual method (copied from https://packagecontrol.io/installation): Click the Preferences > Browse Packages\u2026 menu Browse up a folder and then into the Installed Packages/ folder Download Package Control.sublime-package and copy it into the Installed Packages/ directory Restart Sublime Text","title":"Install Package Control"},{"location":"text_editor_for_epic_modeling/#install-package-development-tool","text":"Go to Preferences --> Package Control . Then type in install until you see \" Package Control: Install Package \" and click on it. Now type PackageDev and click on it. Wait a moment (a progress messages may briefly show in the bottom left of Sublime Text's main window.)","title":"Install Package Development Tool"},{"location":"text_editor_for_epic_modeling/#install-the-syntax-highlighting-definition-for-epics-rule-files","text":"Go to Tools --> Packages --> Package Development --> New Syntax Definition Sublime will give you a sample syntax definition file called \" untitled \". Clear out all the default text. Copy and paste the contents of the Subline Text EPIC-rules Syntax Highlighting File into this window. Now go to the menu and choose File --> Save As . Note that it will automatically point to some folder like Sublime-text/Packages/User , which is the right location! Save the file you just edited into this folder with this exact filename: \" epicrules.sublime-syntax \".","title":"Install the Syntax Highlighting Definition for EPIC's Rule Files"},{"location":"text_editor_for_epic_modeling/#try-it_1","text":"Open up any .prs file, it should automatically be detected as a rule file and you should see colorful syntax highlighting!","title":"Try it"},{"location":"text_editor_for_epic_modeling/#bbedit-text-editor-macos-only","text":"","title":"BBEdit Text Editor [MacOS Only]"},{"location":"text_editor_for_epic_modeling/#downloading-bbedit","text":"There is a paid and free mode of BBEdit. Here is what they say about this on their webpage: BBEdit offers a 30-day evaluation period. During that period, all of BBEdit\u2019s features are available. When it ends, you can still use BBEdit \u2014 with no nag screens or unsolicited interruptions, for free, forever. After the evaluation period, you may re-enable all of BBEdit\u2019s exclusive features at any time by purchasing a license. Download BBEdit from here: http://www.barebones.com/products/bbedit/download.html . Install like any other Mac app.","title":"Downloading BBEdit"},{"location":"text_editor_for_epic_modeling/#settling-up-syntax-highlighting-for-epic-rule-files","text":"Download the BBEdit EPIC-rules Syntax Highlighting File and save it as productionrule.plist . You may need to start and then close BBEdit for the following instructions to work: copy the file productionrule.plist to the folder /Users/YOURUSERNAME/Library/Application Support/BBEdit/Language Modules/ NOTE : If somehow you are using an old copy of TextWrangler instead of BBEdit (which has superseded it), then this will still work if you copy productionrule.plist to this folder instead: /Users/YOURUSERNAME/Library/Application Support/TextWrangler/Language Modules/","title":"Settling Up Syntax Highlighting for EPIC Rule Files"},{"location":"text_editor_for_epic_modeling/#try-it_2","text":"Open up any .prs file, it should automatically be detected as a production-rule file, and you should see colorful syntax highlighting!","title":"Try it"}]}